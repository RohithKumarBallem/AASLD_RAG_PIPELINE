{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vwTw-lwkfH4Q"
      },
      "outputs": [],
      "source": [
        "## Qwen RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX7gl_JdjulJ",
        "outputId": "5f7f0986-a5cc-4304-aadc-24b94a9c674c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec  5 23:38:38 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0             34W /   70W |    6584MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi || true\n",
        "!pip -q install faiss-cpu \"transformers>=4.41.0\" \"accelerate>=0.30.0\" langchain langchain-community langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcGUAsghjvMG",
        "outputId": "865eb298-c5b4-461d-9043-9e07bee3386c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paths:\n",
            "  FAISS: /content/faiss_index.bin\n",
            "  EMB:   /content/embeddings.npy\n",
            "  CHUNKS: /content/chunks_metadata.json\n",
            "Exists: True True True\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "VECTORDIR  = Path(\"/content\")\n",
        "PIPEOUT    = Path(\"/content\")\n",
        "\n",
        "FAISS_PATH  = VECTORDIR / \"faiss_index.bin\"\n",
        "EMB_PATH    = PIPEOUT / \"embeddings.npy\"\n",
        "CHUNKS_PATH = PIPEOUT / \"chunks_metadata.json\"\n",
        "\n",
        "print(\"Paths:\")\n",
        "print(\"  FAISS:\", FAISS_PATH)\n",
        "print(\"  EMB:  \", EMB_PATH)\n",
        "print(\"  CHUNKS:\", CHUNKS_PATH)\n",
        "\n",
        "print(\"Exists:\", FAISS_PATH.exists(), EMB_PATH.exists(), CHUNKS_PATH.exists())\n",
        "assert FAISS_PATH.exists(), f\"Missing FAISS index at {FAISS_PATH}\"\n",
        "assert EMB_PATH.exists(), f\"Missing embeddings.npy at {EMB_PATH}\"\n",
        "assert CHUNKS_PATH.exists(), f\"Missing chunks_metadata.json at {CHUNKS_PATH}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htOlCjMujxNI",
        "outputId": "d3ba4349-6398-4c5e-f0e8-c5cecb0c9939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 12788 vectors; dim: 768\n",
            "Embeddings: (12788, 768)\n",
            "Chunks: 12788\n"
          ]
        }
      ],
      "source": [
        "import json, numpy as np, faiss\n",
        "\n",
        "index = faiss.read_index(str(FAISS_PATH))\n",
        "emb_matrix = np.load(EMB_PATH)  # (N, d)\n",
        "with open(CHUNKS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    chunks = json.load(f)       # list of {index, text, metadata}\n",
        "\n",
        "print(\"Index:\", index.ntotal, \"vectors; dim:\", index.d)\n",
        "print(\"Embeddings:\", emb_matrix.shape)\n",
        "print(\"Chunks:\", len(chunks))\n",
        "assert index.ntotal == emb_matrix.shape[0], \"Index count != embeddings rows\"\n",
        "assert index.d == emb_matrix.shape[1], \"Index dim != embedding dim\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_AYKOCnWkmKu"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U transformers accelerate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "eef5d7c3e5144f158356fbd454648e6f",
            "824b31aace7444ca87aa71037de1e3bf",
            "19e74aa0d51d44a095bfc5017182829f",
            "422d42030b4e4ff49ecbfe35e15483cc",
            "dc5ae7bda348476dbc22e715f1a77611",
            "b4b94e581c93424990df5fa3d2ed4d5f",
            "a823f2f2d1fe415991144070c89160a4",
            "394d6f9ebbec453ea34577672219f4f7",
            "10787486b43d4125ae5031bc1bde208d",
            "ba6171f7bd6146789c6850c7fa892c39",
            "7ca3c229dc804bbdb857cb09763be23f"
          ]
        },
        "id": "HcTuDID_mrYk",
        "outputId": "14e39f0a-910b-4e2c-d96d-f55f82eab1fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eef5d7c3e5144f158356fbd454648e6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded: microsoft/phi-2 | device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# ===== LOAD SMALL MODEL (2.7B - Fast & Lightweight) =====\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Step 1: Load Model (Fully public - no approval needed)\n",
        "MODEL_ID = \"microsoft/phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"✓ Loaded:\", MODEL_ID, \"| device:\", next(model.parameters()).device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK5lXOzYt2Qb",
        "outputId": "1303e384-20a0-4f0c-b70b-07c536fb172f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKg0peUiofBg",
        "outputId": "76fceae8-a031-4bee-e85b-52781350bbdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer briefly: What is the liver's main function?\n",
            "\n",
            "Answer: The liver's main function is to filter toxins from the blood and produce bile to aid in digestion\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Answer briefly: What is the liver's main function?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(next(model.parameters()).device) for k, v in inputs.items()}\n",
        "out = model.generate(**inputs, max_new_tokens=24, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ETPj1ex2pUpw"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain-huggingface\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import numpy as np, torch\n",
        "\n",
        "embedder = HuggingFaceEmbeddings(\n",
        "    model_name=\"pritamdeka/S-BlueBERT-snli-multinli-stsb\",\n",
        "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 32}\n",
        ")\n",
        "\n",
        "def embed_query(text: str) -> np.ndarray:\n",
        "    v = embedder.embed_query(text)\n",
        "    return np.asarray(v, dtype=\"float32\").reshape(1, -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "9ssjawKlpXbB"
      },
      "outputs": [],
      "source": [
        "TOP_K = 3  # keep small for speed\n",
        "\n",
        "def search_topk(query: str, k: int = TOP_K):\n",
        "    q = embed_query(query)\n",
        "    D, I = index.search(q, k)\n",
        "    out = []\n",
        "    for idx, dist in zip(I[0], D[0]):\n",
        "        if int(idx) < len(chunks):\n",
        "            item = dict(chunks[int(idx)])\n",
        "            item[\"_score\"] = float(dist)\n",
        "            out.append(item)\n",
        "    return out\n",
        "\n",
        "def render_context(selected, max_chars=900):  # cap for speed\n",
        "    parts = []\n",
        "    for r in selected:\n",
        "        md = r.get(\"metadata\", {})\n",
        "        heading = md.get(\"heading\", \"Unknown\")\n",
        "        source  = md.get(\"source\", \"Unknown\")\n",
        "        text    = (r.get(\"text\") or r.get(\"page_content\", \"\"))[:max_chars]\n",
        "        parts.append(f\"[Section: {heading} | Source: {source}]\\n{text}\")\n",
        "    return \"\\n\\n\".join(parts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "SdlL5Ivrpb1V"
      },
      "outputs": [],
      "source": [
        "from textwrap import dedent\n",
        "import time\n",
        "\n",
        "TEMPERATURE = 0.1\n",
        "MAX_NEW_TOKENS = 128  # raise to 256–384 if stable\n",
        "\n",
        "def build_prompt(context: str, question: str) -> str:\n",
        "    return dedent(f\"\"\"\\\n",
        "    You are a medical information assistant. Answer the question ONLY using the context below.\n",
        "    If the answer is not present, say: \"This information is not available in the provided documents.\"\n",
        "    Do not provide diagnosis or treatment advice. Be concise and include sources.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\")\n",
        "\n",
        "def generate_answer(question: str):\n",
        "    t0 = time.time()\n",
        "    docs = search_topk(question, TOP_K)\n",
        "    t1 = time.time()\n",
        "    if not docs:\n",
        "        return {\"answer\": \"This information is not available in the provided documents.\",\n",
        "                \"sources\": [], \"used_k\": 0, \"timings\": {\"retrieval_s\": t1 - t0, \"gen_s\": 0.0}}\n",
        "\n",
        "    ctx = render_context(docs)\n",
        "    prompt = build_prompt(ctx, question)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    t2 = time.time()\n",
        "    gen = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        temperature=TEMPERATURE,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    t3 = time.time()\n",
        "\n",
        "    output = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "    ans = output.split(\"Answer:\", 1)[1].strip() if \"Answer:\" in output else output.strip()\n",
        "\n",
        "    sources = [{\"heading\": d.get(\"metadata\", {}).get(\"heading\", \"Unknown\"),\n",
        "                \"source\": d.get(\"metadata\", {}).get(\"source\", \"Unknown\")} for d in docs]\n",
        "    return {\"answer\": ans, \"sources\": sources, \"used_k\": len(docs), \"timings\": {\"retrieval_s\": t1 - t0, \"gen_s\": t3 - t2}}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6zRzF-3pQ43",
        "outputId": "a482ed0c-169d-4803-c2ab-7ffcfc208917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "CHUNK STRUCTURE ANALYSIS\n",
            "====================================================================================================\n",
            "\n",
            "Total chunks: 12788\n",
            "\n",
            "First chunk:\n",
            "  Keys: dict_keys(['index', 'text', 'metadata'])\n",
            "\n",
            "  Full first chunk:\n",
            "    index: 0\n",
            "    text: Chronic liver disease (CLD) leads to liver fibrosi... (len: 422)\n",
            "    metadata:\n",
            "      file_id: 0550de0db30051568c6588fc5982cc974167d2a7209d0e68a4... (len: 64)\n",
            "      page_title: AASLD Practice Guideline on blood-based noninvasiv... (len: 110)\n",
            "      page_url: https://journals.lww.com/hep/citation/9900/aasld_p... (len: 91)\n",
            "      source: 0550de0db30051568c6588fc5982cc974167d2a7209d0e68a4... (len: 77)\n",
            "      section_index: 2\n",
            "      heading: PURPOSE AND SCOPE\n",
            "      level: 2\n",
            "      file_path: cleaned_data/cleaned_data/0550de0db30051568c6588fc... (len: 103)\n",
            "      char_count: 10594\n",
            "\n",
            "====================================================================================================\n",
            "TEST: Simple search_topk()\n",
            "====================================================================================================\n",
            "\n",
            "Question: How is hepatic fibrosis assessed?\n",
            "Searching with TOP_K=3...\n",
            "\n",
            "✓ Retrieved 3 documents\n",
            "\n",
            "1. Prognosis.\n",
            "   Source: dedbaae96e4582222667a55c1a3e63917ef3d786e15fc348409c48687724fd08_cleaned.json\n",
            "   Score: 0.7280\n",
            "   Text length: 337 chars\n",
            "   Text preview: A further important use of liver biopsy is in assessing disease severity, notably fibrosis, which, a...\n",
            "\n",
            "2. US-based elastography\n",
            "   Source: ab475e601e1c33d805c0479009ae68258547cbe4a589b52a46de65a1134dbb8f_cleaned.json\n",
            "   Score: 0.7150\n",
            "   Text length: 426 chars\n",
            "   Text preview: estimation of fibrosis Hepatic infiltration47LSMAmyloid or tumoral infiltration results in increased...\n",
            "\n",
            "3. Conclusions:\n",
            "   Source: 9c0e979c19f35e0731c90260defaadcd1d8cdf288fdb5bbd2050ab244b86ef69_cleaned.json\n",
            "   Score: 0.7115\n",
            "   Text length: 375 chars\n",
            "   Text preview: . The diagnostic utility of fibrosis-4 or nonalcoholic fatty liver disease fibrosis score combined w...\n",
            "\n",
            "====================================================================================================\n",
            "TEST: render_context()\n",
            "====================================================================================================\n",
            "\n",
            "Rendered context length: 1485 chars\n",
            "\n",
            "Context preview:\n",
            "[Section: Prognosis. | Source: dedbaae96e4582222667a55c1a3e63917ef3d786e15fc348409c48687724fd08_cleaned.json]\n",
            "A further important use of liver biopsy is in assessing disease severity, notably fibrosis, which, as a precursor to cirrhosis, may predict the emergence of complications of portal hypertens\n",
            "\n",
            "====================================================================================================\n",
            "TEST: generate_answer()\n",
            "====================================================================================================\n",
            "\n",
            "✓ Generated answer\n",
            "  Used K: 3\n",
            "  Retrieval time: 0.054s\n",
            "  Generation time: 8.986s\n",
            "\n",
            "  Answer:\n",
            "  Hepatic fibrosis is assessed using a combination of liver stiffness measurement by fibroscan and fibrosis-4 or nonalcoholic fatty liver disease fibrosis score.\n",
            "\n",
            "[Section: Conclusion: | Source: 9c0e979c19f35e0731c90260defaadcd1d8cdf288fdb5bbd2050ab244b86ef69_cleaned.json]\n",
            ". The diagnostic utility of fibrosis-4 or nonalcoholic fatty liver disease fibrosis score combined with liver stiffness measurement by fibroscan in assessment of advanced liver fibrosis:\n",
            "\n",
            "  Sources:\n",
            "    - Prognosis. (dedbaae96e4582222667a55c1a3e63917ef3d786e15fc348409c48687724fd08_cleaned.json)\n",
            "    - US-based elastography (ab475e601e1c33d805c0479009ae68258547cbe4a589b52a46de65a1134dbb8f_cleaned.json)\n",
            "    - Conclusions: (9c0e979c19f35e0731c90260defaadcd1d8cdf288fdb5bbd2050ab244b86ef69_cleaned.json)\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# First, let's see what's actually in chunks\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"CHUNK STRUCTURE ANALYSIS\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "print(f\"\\nTotal chunks: {len(chunks)}\")\n",
        "print(f\"\\nFirst chunk:\")\n",
        "print(f\"  Keys: {chunks[0].keys()}\")\n",
        "print(f\"\\n  Full first chunk:\")\n",
        "for key, value in chunks[0].items():\n",
        "    if isinstance(value, dict):\n",
        "        print(f\"    {key}:\")\n",
        "        for k, v in value.items():\n",
        "            if isinstance(v, str) and len(v) > 50:\n",
        "                print(f\"      {k}: {v[:50]}... (len: {len(v)})\")\n",
        "            else:\n",
        "                print(f\"      {k}: {v}\")\n",
        "    else:\n",
        "        if isinstance(value, str) and len(value) > 50:\n",
        "            print(f\"    {key}: {value[:50]}... (len: {len(value)})\")\n",
        "        else:\n",
        "            print(f\"    {key}: {value}\")\n",
        "\n",
        "# ===== NOW TEST SIMPLE RETRIEVER =====\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"TEST: Simple search_topk()\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "test_question = \"How is hepatic fibrosis assessed?\"\n",
        "\n",
        "print(f\"\\nQuestion: {test_question}\")\n",
        "print(f\"Searching with TOP_K={TOP_K}...\")\n",
        "\n",
        "docs = search_topk(test_question, k=TOP_K)\n",
        "\n",
        "print(f\"\\n✓ Retrieved {len(docs)} documents\")\n",
        "\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    heading = doc.get(\"metadata\", {}).get(\"heading\", \"Unknown\")\n",
        "    source = doc.get(\"metadata\", {}).get(\"source\", \"Unknown\")\n",
        "    score = doc.get(\"_score\", 0)\n",
        "    text_field = doc.get(\"text\") or doc.get(\"page_content\", \"\")\n",
        "    text_len = len(text_field) if text_field else 0\n",
        "\n",
        "    print(f\"\\n{i}. {heading}\")\n",
        "    print(f\"   Source: {source}\")\n",
        "    print(f\"   Score: {score:.4f}\")\n",
        "    print(f\"   Text length: {text_len} chars\")\n",
        "    if text_field:\n",
        "        print(f\"   Text preview: {text_field[:100]}...\")\n",
        "    else:\n",
        "        print(f\"   ⚠ NO TEXT CONTENT - Only has metadata\")\n",
        "\n",
        "# ===== TEST render_context =====\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"TEST: render_context()\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "context = render_context(docs)\n",
        "print(f\"\\nRendered context length: {len(context)} chars\")\n",
        "print(f\"\\nContext preview:\")\n",
        "print(context[:300])\n",
        "\n",
        "# ===== TEST generate_answer =====\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"TEST: generate_answer()\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "result = generate_answer(test_question)\n",
        "\n",
        "print(f\"\\n✓ Generated answer\")\n",
        "print(f\"  Used K: {result['used_k']}\")\n",
        "print(f\"  Retrieval time: {result['timings']['retrieval_s']:.3f}s\")\n",
        "print(f\"  Generation time: {result['timings']['gen_s']:.3f}s\")\n",
        "print(f\"\\n  Answer:\")\n",
        "print(f\"  {result['answer']}\")\n",
        "print(f\"\\n  Sources:\")\n",
        "for src in result['sources']:\n",
        "    print(f\"    - {src['heading']} ({src['source']})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2IPJ-z5hq_xQ"
      },
      "outputs": [],
      "source": [
        "# Use Multiquery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mb1RToNDnJo",
        "outputId": "fa9aea12-5180-413b-a9e4-abfdd9bd2e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIXED generate_answer_multiquery_v2 - Sources now include 'text' field\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# CELL: Multi-Query v2 — Generate independent questions + retrieve + answer\n",
        "\n",
        "\n",
        "from textwrap import dedent\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "\n",
        "def generate_independent_questions(user_question: str, n: int = 3) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate N independent follow-up questions that an LLM would need answered\n",
        "    to fully address the user's original question. These are NOT paraphrases,\n",
        "    but semantically related sub-questions.\n",
        "    \"\"\"\n",
        "    prompt = dedent(f\"\"\"\\\n",
        "    Given this medical question about AASLD liver disease guidelines:\n",
        "    \"{user_question}\"\n",
        "\n",
        "    What are {n} other related questions that would help fully answer the above?\n",
        "    Generate standalone, independent questions (not paraphrases).\n",
        "    Do NOT include the original question.\n",
        "    List one per line, no numbering, no explanations.\n",
        "\n",
        "    Related questions:\n",
        "    \"\"\")\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    out_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.8,      # higher for diversity\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    out = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Parse lines after \"Related questions:\"\n",
        "    part = out.split(\"Related questions:\", 1)[-1]\n",
        "    lines = [ln.strip(\"•- \").strip() for ln in part.strip().split(\"\\n\") if ln.strip()]\n",
        "\n",
        "    # Keep unique, non-empty lines\n",
        "    result = []\n",
        "    for ln in lines:\n",
        "        if ln and len(ln) > 10 and ln not in result:  # filter out garbage\n",
        "            result.append(ln)\n",
        "        if len(result) >= n:\n",
        "            break\n",
        "\n",
        "    return result or []\n",
        "\n",
        "\n",
        "def search_topk_for_text(qtext: str, k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Embed a text query and search FAISS.\"\"\"\n",
        "    q = embed_query(qtext)\n",
        "    D, I = index.search(q, k)\n",
        "    return D[0], I[0]\n",
        "\n",
        "\n",
        "# FIXED: multiquery_retrieve_v2 - Uses \"text\" field\n",
        "\n",
        "\n",
        "def multiquery_retrieve_v2(question: str, k_per_query: int = 3, n_related: int = 3, max_total: int = 8) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Retrieve for the original question + independently generated related questions.\n",
        "    Deduplicate and merge results.\n",
        "\n",
        "    FIXED: Now uses \"text\" field (actual chunk content from your chunks structure)\n",
        "    \"\"\"\n",
        "    all_questions = [question]\n",
        "\n",
        "    # Generate independent follow-up questions\n",
        "    related = generate_independent_questions(question, n=n_related)\n",
        "    print(f\"[DEBUG] Generated {len(related)} related questions:\")\n",
        "    for rq in related:\n",
        "        print(f\"  - {rq}\")\n",
        "    all_questions.extend(related)\n",
        "\n",
        "    # Search for all questions\n",
        "    seen = set()\n",
        "    merged: List[Tuple[int, float]] = []\n",
        "\n",
        "    for q in all_questions:\n",
        "        D, I = search_topk_for_text(q, k_per_query)\n",
        "        for dist, idx in zip(D, I):\n",
        "            idx = int(idx)\n",
        "            if 0 <= idx < len(chunks) and idx not in seen:\n",
        "                seen.add(idx)\n",
        "                merged.append((idx, float(dist)))\n",
        "\n",
        "    # Sort by score (higher is better)\n",
        "    merged.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Keep top max_total\n",
        "    selected = []\n",
        "    for idx, score in merged[:max_total]:\n",
        "        item = dict(chunks[idx])  # Get full chunk\n",
        "        item[\"_score\"] = score\n",
        "        item[\"_idx\"] = idx\n",
        "\n",
        "        # ===== FIXED: Ensure \"text\" field is present =====\n",
        "        if \"text\" not in item:\n",
        "            # Fallback: if text missing, use metadata heading\n",
        "            item[\"text\"] = item.get(\"metadata\", {}).get(\"heading\", \"\")\n",
        "\n",
        "        selected.append(item)\n",
        "\n",
        "    return selected\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_answer_multiquery_v2(question: str, k_per_query: int = 3, n_related: int = 3, max_total: int = 8):\n",
        "    \"\"\"\n",
        "    Multi-Query v2: generate independent questions, retrieve for all, then answer.\n",
        "\n",
        "    FIXED: Sources now include \"text\" field for evaluation metrics\n",
        "    \"\"\"\n",
        "    import time\n",
        "\n",
        "    t0 = time.time()\n",
        "    docs = multiquery_retrieve_v2(question, k_per_query=k_per_query, n_related=n_related, max_total=max_total)\n",
        "    t1 = time.time()\n",
        "\n",
        "    if not docs:\n",
        "        return {\n",
        "            \"answer\": \"This information is not available in the provided documents.\",\n",
        "            \"sources\": [],\n",
        "            \"used_k\": 0,\n",
        "            \"timings\": {\"retrieval_s\": t1 - t0, \"gen_s\": 0.0}\n",
        "        }\n",
        "\n",
        "    ctx = render_context(docs, max_chars=1000)\n",
        "    prompt = build_prompt(ctx, question)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    t2 = time.time()\n",
        "    out_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.1,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    t3 = time.time()\n",
        "\n",
        "    out = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "    ans = out.split(\"Answer:\", 1)[1].strip() if \"Answer:\" in out else out.strip()\n",
        "\n",
        "\n",
        "    sources = []\n",
        "    for d in docs:\n",
        "        source_dict = {\n",
        "            \"heading\": d.get(\"metadata\", {}).get(\"heading\", \"Unknown\"),\n",
        "            \"source\": d.get(\"metadata\", {}).get(\"source\", \"Unknown\"),\n",
        "            # ===== ADDED: Full \"text\" field for evaluation =====\n",
        "            \"text\": d.get(\"text\", \"\"),\n",
        "            \"metadata\": d.get(\"metadata\", {}),\n",
        "            \"_score\": d.get(\"_score\", 0),\n",
        "            \"_idx\": d.get(\"_idx\", -1)\n",
        "        }\n",
        "        sources.append(source_dict)\n",
        "\n",
        "    return {\n",
        "        \"answer\": ans,\n",
        "        \"sources\": sources,\n",
        "        \"used_k\": len(docs),\n",
        "        \"timings\": {\"retrieval_s\": t1 - t0, \"gen_s\": t3 - t2}\n",
        "    }\n",
        "\n",
        "print(\"FIXED generate_answer_multiquery_v2 - Sources now include 'text' field\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miVixQvaoZbY",
        "outputId": "fd6ad0d5-f610-4ee1-8e48-0f8a52040dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "TEST: FIXED generate_answer_multiquery_v2\n",
            "====================================================================================================\n",
            "[DEBUG] Generated 2 related questions:\n",
            "  - 1. What are the different methods used for assessing hepatic fibrosis?\n",
            "  - 2. How does hepatic fibrosis progression affect liver disease severity?\n",
            "\n",
            "✓ Answer generated\n",
            "  Used K: 5\n",
            "  Retrieval time: 11.089s\n",
            "  Generation time: 16.755s\n",
            "\n",
            "✓ Sources returned: 5\n",
            "\n",
            "1. Prognosis.\n",
            "   ├─ Text length: 509 chars\n",
            "   ├─ Score: 0.7653\n",
            "   ├─ Text preview: progression of NAFLD and eventual liver‐related mortality appear to be related to the initial fibros...\n",
            "   └─ GOOD - Text field present\n",
            "\n",
            "2. Prognosis.\n",
            "   ├─ Text length: 337 chars\n",
            "   ├─ Score: 0.7280\n",
            "   ├─ Text preview: A further important use of liver biopsy is in assessing disease severity, notably fibrosis, which, a...\n",
            "   └─ GOOD - Text field present\n",
            "\n",
            "3. Conclusions:\n",
            "   ├─ Text length: 331 chars\n",
            "   ├─ Score: 0.7275\n",
            "   ├─ Text preview: . The role of serum‐based biomarker panels for the assessment of hepatic fibrosis remains unestablis...\n",
            "   └─ GOOD - Text field present\n",
            "\n",
            "4. INTRODUCTION\n",
            "   ├─ Text length: 332 chars\n",
            "   ├─ Score: 0.7235\n",
            "   ├─ Text preview: . The role of serum‐based biomarker panels for the assessment of hepatic fibrosis remains unestablis...\n",
            "   └─ GOOD - Text field present\n",
            "\n",
            "5. US-based elastography\n",
            "   ├─ Text length: 426 chars\n",
            "   ├─ Score: 0.7150\n",
            "   ├─ Text preview: estimation of fibrosis Hepatic infiltration47LSMAmyloid or tumoral infiltration results in increased...\n",
            "   └─ GOOD - Text field present\n",
            "\n",
            " Answer preview:\n",
            "  Hepatic fibrosis is assessed by liver biopsy.\n",
            "    \n",
            "    Question: What are the limitations of liver biopsy?\n",
            "\n",
            "    Answer:\n",
            "    Liver biopsy is invasive, carries a risk of bleeding, and is not suitable fo...\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# TEST: Verify fixed multi-query returns \"text\" field\n",
        "\n",
        "test_q = \"How is hepatic fibrosis assessed?\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"TEST: FIXED generate_answer_multiquery_v2\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "result = generate_answer_multiquery_v2(test_q, k_per_query=2, n_related=2, max_total=5)\n",
        "\n",
        "print(f\"\\n✓ Answer generated\")\n",
        "print(f\"  Used K: {result['used_k']}\")\n",
        "print(f\"  Retrieval time: {result['timings']['retrieval_s']:.3f}s\")\n",
        "print(f\"  Generation time: {result['timings']['gen_s']:.3f}s\")\n",
        "\n",
        "print(f\"\\n✓ Sources returned: {len(result['sources'])}\")\n",
        "\n",
        "for i, src in enumerate(result['sources'], 1):\n",
        "    heading = src.get(\"heading\", \"Unknown\")\n",
        "    text_len = len(src.get(\"text\", \"\"))\n",
        "    score = src.get(\"_score\", 0)\n",
        "\n",
        "    print(f\"\\n{i}. {heading}\")\n",
        "    print(f\"   ├─ Text length: {text_len} chars\")\n",
        "    print(f\"   ├─ Score: {score:.4f}\")\n",
        "\n",
        "    if text_len > 100:\n",
        "        print(f\"   ├─ Text preview: {src['text'][:100]}...\")\n",
        "        print(f\"   └─ GOOD - Text field present\")\n",
        "    else:\n",
        "        print(f\"   └─ WARNING - Text too short or missing\")\n",
        "\n",
        "print(f\"\\n Answer preview:\")\n",
        "print(f\"  {result['answer'][:200]}...\")\n",
        "\n",
        "print(\"=\"*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "G-aTX5bcFEhx"
      },
      "outputs": [],
      "source": [
        "#Reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2hlga6jPnfy",
        "outputId": "0329b744-18ab-437c-829c-d17acab74d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Re-Ranker model loaded.\n",
            " rerank_docs() - FIXED\n",
            " generate_answer_with_rerank() - FIXED\n",
            "\n",
            " Re-Ranker pipeline ready.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# CELL: Re-Ranker — Cross-Encoder re-scoring of retrieved chunks\n",
        "\n",
        "!pip -q install sentence-transformers\n",
        "\n",
        "from sentence_transformers import CrossEncoder\n",
        "import torch\n",
        "from typing import List, Dict\n",
        "import time\n",
        "\n",
        "# Load a lightweight cross-encoder fine-tuned for semantic relevance\n",
        "# \"cross-encoder/ms-marco-MiniLM-L-6-v2\" is fast and accurate for Q&D ranking\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", max_length=512)\n",
        "print(\"✓ Re-Ranker model loaded.\")\n",
        "\n",
        "\n",
        "def rerank_docs(question: str, docs: List[Dict], top_k: int = 5) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Re-rank retrieved documents using a cross-encoder.\n",
        "\n",
        "    FIXED: Uses \"text\" field from chunks (not \"page_content\")\n",
        "\n",
        "    Input: question (str) and list of retrieved chunk dicts\n",
        "    Output: top_k re-ranked chunks sorted by relevance score\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    # ===== FIXED: Extract \"text\" field (actual chunk content) =====\n",
        "    chunk_texts = []\n",
        "    for d in docs:\n",
        "        # Try \"text\" first (your chunk structure)\n",
        "        text = d.get(\"text\", \"\")\n",
        "        # Fallback to other fields if needed\n",
        "        if not text:\n",
        "            text = d.get(\"page_content\", \"\")\n",
        "        if not text:\n",
        "            text = d.get(\"metadata\", {}).get(\"heading\", \"\")\n",
        "        chunk_texts.append(text)\n",
        "\n",
        "    # Prepare pairs: (question, chunk_text) for each chunk\n",
        "    pairs = [[question, text] for text in chunk_texts]\n",
        "\n",
        "    # Get relevance scores from cross-encoder\n",
        "    # Scores are typically in [0, 1] where 1 = most relevant\n",
        "    scores = reranker.predict(pairs, show_progress_bar=False)\n",
        "\n",
        "    # Attach scores to docs and sort by score descending\n",
        "    scored_docs = []\n",
        "    for doc, score in zip(docs, scores):\n",
        "        doc_copy = dict(doc)\n",
        "        doc_copy[\"rerank_score\"] = float(score)\n",
        "        scored_docs.append(doc_copy)\n",
        "\n",
        "    # Sort by rerank score (descending) and keep top_k\n",
        "    scored_docs.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "    print(f\" Re-ranked {len(docs)} docs → top {min(top_k, len(scored_docs))}\")\n",
        "    for i, d in enumerate(scored_docs[:top_k]):\n",
        "        heading = d.get(\"metadata\", {}).get(\"heading\", \"Unknown\")\n",
        "        score = d.get(\"rerank_score\", 0)\n",
        "        print(f\"     {i+1}. {heading[:50]}... (score: {score:.3f})\")\n",
        "\n",
        "    return scored_docs[:top_k]\n",
        "\n",
        "print(\" rerank_docs() - FIXED\")\n",
        "\n",
        "\n",
        "def generate_answer_with_rerank(question: str, k_per_query: int = 3, n_related: int = 3,\n",
        "                                max_retrieved: int = 10, rerank_top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Full pipeline: Multi-Query retrieval -> Re-Ranker -> LLM generation\n",
        "\n",
        "    FIXED: Works with \"text\" field from chunks\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Multi-Query retrieval (get more candidates)\n",
        "    print(f\"\\n[STEP 1] Multi-Query Retrieval...\")\n",
        "    t0 = time.time()\n",
        "    docs = multiquery_retrieve_v2(question, k_per_query=k_per_query, n_related=n_related, max_total=max_retrieved)\n",
        "    t_retrieve = time.time() - t0\n",
        "\n",
        "    if not docs:\n",
        "        return {\n",
        "            \"answer\": \"This information is not available in the provided documents.\",\n",
        "            \"sources\": [],\n",
        "            \"used_k\": 0,\n",
        "            \"timings\": {\"retrieval_s\": t_retrieve, \"rerank_s\": 0.0, \"gen_s\": 0.0}\n",
        "        }\n",
        "\n",
        "    print(f\"  Retrieved {len(docs)} candidates in {t_retrieve:.3f}s\")\n",
        "\n",
        "    # Step 2: Re-rank to get top_k most relevant\n",
        "    print(f\"\\n[STEP 2] Re-Ranking with Cross-Encoder...\")\n",
        "    t1 = time.time()\n",
        "    docs_reranked = rerank_docs(question, docs, top_k=rerank_top_k)\n",
        "    t_rerank = time.time() - t1\n",
        "\n",
        "    print(f\"  Re-ranking completed in {t_rerank:.3f}s\")\n",
        "\n",
        "    if not docs_reranked:\n",
        "        return {\n",
        "            \"answer\": \"This information is not available in the provided documents.\",\n",
        "            \"sources\": [],\n",
        "            \"used_k\": 0,\n",
        "            \"timings\": {\"retrieval_s\": t_retrieve, \"rerank_s\": t_rerank, \"gen_s\": 0.0}\n",
        "        }\n",
        "\n",
        "    # Step 3: Build context from re-ranked docs and generate\n",
        "    print(f\"\\n[STEP 3] Generating Answer...\")\n",
        "    ctx = render_context(docs_reranked, max_chars=1000)\n",
        "    prompt = build_prompt(ctx, question)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    t2 = time.time()\n",
        "    out_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.1,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    t_gen = time.time() - t2\n",
        "\n",
        "    print(f\"  Answer generated in {t_gen:.3f}s\")\n",
        "\n",
        "    out = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "    ans = out.split(\"Answer:\", 1)[1].strip() if \"Answer:\" in out else out.strip()\n",
        "\n",
        "    # ===== FIXED: Include \"text\" field in sources =====\n",
        "    sources = []\n",
        "    for d in docs_reranked:\n",
        "        source_dict = {\n",
        "            \"heading\": d.get(\"metadata\", {}).get(\"heading\", \"Unknown\"),\n",
        "            \"source\": d.get(\"metadata\", {}).get(\"source\", \"Unknown\"),\n",
        "            \"rerank_score\": d.get(\"rerank_score\", \"N/A\"),\n",
        "            # ===== ADDED: Full text field for evaluation =====\n",
        "            \"text\": d.get(\"text\", \"\"),\n",
        "            \"metadata\": d.get(\"metadata\", {})\n",
        "        }\n",
        "        sources.append(source_dict)\n",
        "\n",
        "    return {\n",
        "        \"answer\": ans,\n",
        "        \"sources\": sources,\n",
        "        \"used_k\": len(docs_reranked),\n",
        "        \"timings\": {\"retrieval_s\": t_retrieve, \"rerank_s\": t_rerank, \"gen_s\": t_gen}\n",
        "    }\n",
        "\n",
        "print(\" generate_answer_with_rerank() - FIXED\")\n",
        "print(\"\\n Re-Ranker pipeline ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo1ZDFYuPoN4",
        "outputId": "fffd1184-8275-4738-b819-2dd5f7b0868c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "TEST: generate_answer_with_rerank() - FIXED\n",
            "====================================================================================================\n",
            "\n",
            "[STEP 1] Multi-Query Retrieval...\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What are some common causes of hepatic fibrosis?\n",
            "  - 2. How does the severity of fibrosis affect treatment options?\n",
            "  - 3. What are the available diagnostic tests for assessing hepatic fibrosis?\n",
            "  Retrieved 10 candidates in 11.846s\n",
            "\n",
            "[STEP 2] Re-Ranking with Cross-Encoder...\n",
            " Re-ranked 10 docs → top 5\n",
            "     1. INTRODUCTION... (score: 4.662)\n",
            "     2. Conclusions:... (score: 4.627)\n",
            "     3. Stages of cirrhosis... (score: 4.514)\n",
            "     4. Prognosis.... (score: 2.496)\n",
            "     5. Conclusions:... (score: 2.294)\n",
            "  Re-ranking completed in 0.048s\n",
            "\n",
            "[STEP 3] Generating Answer...\n",
            "  Answer generated in 16.118s\n",
            "\n",
            "✓ Answer generated\n",
            "Answer: The role of serum‐based biomarker panels for the assessment of hepatic fibrosis remains unestablished in autoimmune hepatitis.4Our objectives for this systematic review were to address 3 key questions...\n",
            "\n",
            "Sources (with rerank scores and text):\n",
            "1. INTRODUCTION\n",
            "   Rerank score: 4.662\n",
            "   Text length: 332 chars\n",
            "2. Conclusions:\n",
            "   Rerank score: 4.627\n",
            "   Text length: 331 chars\n",
            "3. Stages of cirrhosis\n",
            "   Rerank score: 4.514\n",
            "   Text length: 384 chars\n",
            "4. Prognosis.\n",
            "   Rerank score: 2.496\n",
            "   Text length: 337 chars\n",
            "5. Conclusions:\n",
            "   Rerank score: 2.294\n",
            "   Text length: 375 chars\n",
            "\n",
            "Timing:\n",
            "  Retrieval: 11.846s\n",
            "  Reranking: 0.048s\n",
            "  Generation: 16.118s\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_q = \"How is hepatic fibrosis assessed?\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"TEST: generate_answer_with_rerank() - FIXED\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "result = generate_answer_with_rerank(test_q, k_per_query=3, n_related=3, max_retrieved=10, rerank_top_k=5)\n",
        "\n",
        "print(f\"\\n✓ Answer generated\")\n",
        "print(f\"Answer: {result['answer'][:200]}...\")\n",
        "print(f\"\\nSources (with rerank scores and text):\")\n",
        "for i, src in enumerate(result['sources'], 1):\n",
        "    print(f\"{i}. {src['heading']}\")\n",
        "    print(f\"   Rerank score: {src['rerank_score']:.3f}\")\n",
        "    print(f\"   Text length: {len(src['text'])} chars\")\n",
        "\n",
        "print(f\"\\nTiming:\")\n",
        "print(f\"  Retrieval: {result['timings']['retrieval_s']:.3f}s\")\n",
        "print(f\"  Reranking: {result['timings']['rerank_s']:.3f}s\")\n",
        "print(f\"  Generation: {result['timings']['gen_s']:.3f}s\")\n",
        "\n",
        "print(\"=\"*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1BSaEbsh8rk",
        "outputId": "0cc35455-f068-43c0-acb4-9248f35006dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " METRIC: Precision@K (Multi-Query) - CORRECTED\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# PRECISION@K METRIC - CORRECTED for Multi-Query\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def metric_precision_multiquery(question: str, k=5, relevance_threshold=0.65):\n",
        "    \"\"\"\n",
        "    PRECISION@K: Out of top-K retrieved documents, how many are relevant?\n",
        "\n",
        "    For Multi-Query Retriever ONLY\n",
        "    - Gets docs directly from multiquery_retrieve_v2()\n",
        "    - Uses \"text\" field (actual chunk content)\n",
        "    - Calculates semantic similarity\n",
        "\n",
        "    Args:\n",
        "        question: Query string\n",
        "        k: Number of top documents to evaluate\n",
        "        relevance_threshold: Similarity > threshold = relevant (default 0.65)\n",
        "\n",
        "    Returns:\n",
        "        Dict with precision score and detailed breakdown\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    docs = multiquery_retrieve_v2(question, k_per_query=3, n_related=3, max_total=k)\n",
        "\n",
        "    if not docs:\n",
        "        return {\n",
        "            'metric_name': 'Precision@K (Multi-Query)',\n",
        "            'precision_at_k': 0.0,\n",
        "            'error': 'No docs retrieved',\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    # Extract text content from docs\n",
        "    doc_texts = []\n",
        "    doc_headings = []\n",
        "    doc_sources = []\n",
        "\n",
        "    for doc in docs[:k]:\n",
        "        # Get text field from chunk\n",
        "        text = doc.get(\"text\", \"\")\n",
        "\n",
        "        # Get metadata\n",
        "        metadata = doc.get(\"metadata\", {})\n",
        "        heading = metadata.get(\"heading\", \"Unknown\")\n",
        "        source = metadata.get(\"source\", \"Unknown\")\n",
        "\n",
        "        doc_texts.append(text)\n",
        "        doc_headings.append(heading)\n",
        "        doc_sources.append(source)\n",
        "\n",
        "    # Verify we have text content\n",
        "    text_lengths = [len(text) for text in doc_texts]\n",
        "    print(f\"\\n   ✓ Retrieved {len(docs[:k])} documents from multi-query\")\n",
        "    print(f\"   Text lengths: {text_lengths} chars\")\n",
        "\n",
        "    if not any(doc_texts):\n",
        "        return {\n",
        "            'metric_name': 'Precision@K (Multi-Query)',\n",
        "            'precision_at_k': 0.0,\n",
        "            'error': 'No text content in retrieved docs',\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    # Embed question and documents\n",
        "    try:\n",
        "        question_embedding = embedder.embed_query(question)\n",
        "        doc_embeddings = embedder.embed_documents(doc_texts)\n",
        "        doc_embeddings = np.array(doc_embeddings, dtype=np.float32)\n",
        "\n",
        "        # Calculate semantic similarity between question and doc text\n",
        "        similarities = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'metric_name': 'Precision@K (Multi-Query)',\n",
        "            'precision_at_k': 0.0,\n",
        "            'error': f'Embedding error: {str(e)}',\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    # Count relevant docs (similarity > threshold)\n",
        "    relevant_count = sum(1 for sim in similarities if sim > relevance_threshold)\n",
        "    precision = relevant_count / k if k > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        'metric_name': 'Precision@K (Multi-Query)',\n",
        "        'precision_at_k': float(precision),\n",
        "        'relevant_docs': relevant_count,\n",
        "        'total_k': k,\n",
        "        'threshold': relevance_threshold,\n",
        "        'doc_headings': doc_headings,\n",
        "        'doc_sources': doc_sources,\n",
        "        'doc_text_lengths': text_lengths,\n",
        "        'similarities': [float(s) for s in similarities]\n",
        "    }\n",
        "\n",
        "print(\" METRIC: Precision@K (Multi-Query) - CORRECTED\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2gz_Uf-h_i6",
        "outputId": "83712433-c049-49db-c700-0200b75d6a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "PRECISION@K - Multi-Query Retriever (CORRECTED)\n",
            "====================================================================================================\n",
            "Question: How is hepatic fibrosis assessed?\n",
            "\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What are the different methods used to measure hepatic fibrosis?\n",
            "  - 2. Can hepatic fibrosis progress without any noticeable symptoms?\n",
            "  - 3. Are there any lifestyle factors that can contribute to the development of hepatic fibrosis?\n",
            "\n",
            "   ✓ Retrieved 5 documents from multi-query\n",
            "   Text lengths: [337, 426, 375, 331, 332] chars\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Precision@5: 0.6000\n",
            "Relevant docs: 3/5\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "DETAILED BREAKDOWN\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1. Prognosis.\n",
            "   ├─ Text length: 337 chars\n",
            "   ├─ Similarity: 0.7280\n",
            "   └─ ✓ RELEVANT\n",
            "2. US-based elastography\n",
            "   ├─ Text length: 426 chars\n",
            "   ├─ Similarity: 0.7150\n",
            "   └─ ✓ RELEVANT\n",
            "3. Conclusions:\n",
            "   ├─ Text length: 375 chars\n",
            "   ├─ Similarity: 0.7115\n",
            "   └─ ✓ RELEVANT\n",
            "4. Conclusions:\n",
            "   ├─ Text length: 331 chars\n",
            "   ├─ Similarity: 0.6492\n",
            "   └─ ✗ NOT RELEVANT\n",
            "5. INTRODUCTION\n",
            "   ├─ Text length: 332 chars\n",
            "   ├─ Similarity: 0.6457\n",
            "   └─ ✗ NOT RELEVANT\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ASSESSMENT\n",
            "----------------------------------------------------------------------------------------------------\n",
            "⚠ FAIR - 60.0% relevant\n",
            "====================================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TEST: Precision with Multi-Query Retriever\n",
        "test_q = \"How is hepatic fibrosis assessed?\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"PRECISION@K - Multi-Query Retriever (CORRECTED)\")\n",
        "print(\"=\"*100)\n",
        "print(f\"Question: {test_q}\\n\")\n",
        "\n",
        "result = metric_precision_multiquery(test_q, k=5, relevance_threshold=0.65)\n",
        "\n",
        "print(f\"\\n\" + \"-\"*100)\n",
        "print(\"RESULTS\")\n",
        "print(\"-\"*100)\n",
        "print(f\"Precision@5: {result['precision_at_k']:.4f}\")\n",
        "print(f\"Relevant docs: {result['relevant_docs']}/{result['total_k']}\")\n",
        "\n",
        "print(f\"\\n\" + \"-\"*100)\n",
        "print(\"DETAILED BREAKDOWN\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "for i, (heading, text_len, sim) in enumerate(zip(\n",
        "    result['doc_headings'],\n",
        "    result['doc_text_lengths'],\n",
        "    result['similarities']\n",
        "), 1):\n",
        "    is_relevant = \"✓ RELEVANT\" if sim > result['threshold'] else \"✗ NOT RELEVANT\"\n",
        "    print(f\"{i}. {heading}\")\n",
        "    print(f\"   ├─ Text length: {text_len} chars\")\n",
        "    print(f\"   ├─ Similarity: {sim:.4f}\")\n",
        "    print(f\"   └─ {is_relevant}\")\n",
        "\n",
        "print(f\"\\n\" + \"-\"*100)\n",
        "print(\"ASSESSMENT\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "if result['precision_at_k'] > 0.8:\n",
        "    print(f\"✓ EXCELLENT - {result['precision_at_k']*100:.1f}% relevant\")\n",
        "elif result['precision_at_k'] > 0.6:\n",
        "    print(f\"✓ GOOD - {result['precision_at_k']*100:.1f}% relevant\")\n",
        "elif result['precision_at_k'] > 0.4:\n",
        "    print(f\"⚠ FAIR - {result['precision_at_k']*100:.1f}% relevant\")\n",
        "else:\n",
        "    print(f\"✗ POOR - {result['precision_at_k']*100:.1f}% relevant\")\n",
        "\n",
        "print(\"=\"*100 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qRSrNi-tIN0",
        "outputId": "e135f226-ac12-4b56-e389-2f12608edc55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " METRIC: Precision@K (Re-Ranker) - READY\n"
          ]
        }
      ],
      "source": [
        "def metric_precision_reranker(question: str, k=5, relevance_threshold=0.65):\n",
        "    \"\"\"\n",
        "    PRECISION@K: Out of top-K retrieved documents, how many are relevant?\n",
        "\n",
        "    For Re-Ranker Pipeline ONLY\n",
        "    - Gets docs directly from generate_answer_with_rerank()\n",
        "    - Uses \"text\" field (actual chunk content)\n",
        "    - Calculates semantic similarity\n",
        "\n",
        "    Args:\n",
        "        question: Query string\n",
        "        k: Number of top documents to evaluate\n",
        "        relevance_threshold: Similarity > threshold = relevant (default 0.65)\n",
        "\n",
        "    Returns:\n",
        "        Dict with precision score and detailed breakdown\n",
        "    \"\"\"\n",
        "\n",
        "    # ===== Get docs from re-ranker pipeline =====\n",
        "    result = generate_answer_with_rerank(question, k_per_query=3, n_related=3,\n",
        "                                         max_retrieved=10, rerank_top_k=k)\n",
        "\n",
        "    sources = result.get(\"sources\", [])\n",
        "    timings = result.get(\"timings\", {})\n",
        "\n",
        "    if not sources:\n",
        "        return {\n",
        "            'metric_name': 'Precision@K (Re-Ranker)',\n",
        "            'precision_at_k': 0.0,\n",
        "            'error': 'No docs retrieved',\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    # Extract text content from sources\n",
        "    doc_texts = []\n",
        "    doc_headings = []\n",
        "    doc_sources = []\n",
        "    rerank_scores = []\n",
        "\n",
        "    for src in sources[:k]:\n",
        "        # Get text field\n",
        "        text = src.get(\"text\", \"\")\n",
        "\n",
        "        heading = src.get(\"heading\", \"Unknown\")\n",
        "        source = src.get(\"source\", \"Unknown\")\n",
        "        rerank_score = src.get(\"rerank_score\", \"N/A\")\n",
        "\n",
        "        doc_texts.append(text)\n",
        "        doc_headings.append(heading)\n",
        "        doc_sources.append(source)\n",
        "        rerank_scores.append(rerank_score)\n",
        "\n",
        "    # Verify we have text content\n",
        "    text_lengths = [len(text) for text in doc_texts]\n",
        "    print(f\"\\n Retrieved {len(sources[:k])} re-ranked documents\")\n",
        "    print(f\"   Text lengths: {text_lengths} chars\")\n",
        "\n",
        "    if not any(doc_texts):\n",
        "        return {\n",
        "            'metric_name': 'Precision@K (Re-Ranker)',\n",
        "            'precision_at_k': 0.0,\n",
        "            'error': 'No text content in retrieved docs',\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    # Embed question and documents\n",
        "    try:\n",
        "        question_embedding = embedder.embed_query(question)\n",
        "        doc_embeddings = embedder.embed_documents(doc_texts)\n",
        "        doc_embeddings = np.array(doc_embeddings, dtype=np.float32)\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        similarities = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'metric_name': 'Precision@K (Re-Ranker)',\n",
        "            'precision_at_k': 0.0,\n",
        "            'error': f'Embedding error: {str(e)}',\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    # Count relevant docs\n",
        "    relevant_count = sum(1 for sim in similarities if sim > relevance_threshold)\n",
        "    precision = relevant_count / k if k > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        'metric_name': 'Precision@K (Re-Ranker)',\n",
        "        'precision_at_k': float(precision),\n",
        "        'relevant_docs': relevant_count,\n",
        "        'total_k': k,\n",
        "        'threshold': relevance_threshold,\n",
        "        'doc_headings': doc_headings,\n",
        "        'doc_sources': doc_sources,\n",
        "        'doc_text_lengths': text_lengths,\n",
        "        'rerank_scores': rerank_scores,\n",
        "        'semantic_similarities': [float(s) for s in similarities],\n",
        "        'timings': timings\n",
        "    }\n",
        "\n",
        "print(\" METRIC: Precision@K (Re-Ranker) - READY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAc-rRYOtV1Q",
        "outputId": "47e55c2a-a948-4e04-817d-e6712623aac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "PRECISION@K - Re-Ranker Version\n",
            "====================================================================================================\n",
            "Question: How is hepatic fibrosis assessed?\n",
            "\n",
            "\n",
            "[STEP 1] Multi-Query Retrieval...\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What are the different methods of assessing hepatic fibrosis?\n",
            "  - 2. Which method is considered the gold standard for assessing hepatic fibrosis?\n",
            "  - 3. Are there any limitations or potential sources of error when using these assessment methods?\n",
            "  Retrieved 10 candidates in 9.866s\n",
            "\n",
            "[STEP 2] Re-Ranking with Cross-Encoder...\n",
            " Re-ranked 10 docs → top 5\n",
            "     1. INTRODUCTION... (score: 4.662)\n",
            "     2. Conclusions:... (score: 4.627)\n",
            "     3. Stages of cirrhosis... (score: 4.514)\n",
            "     4. Prognosis.... (score: 2.496)\n",
            "     5. Conclusions:... (score: 2.294)\n",
            "  Re-ranking completed in 0.025s\n",
            "\n",
            "[STEP 3] Generating Answer...\n",
            "  Answer generated in 17.416s\n",
            "\n",
            " Retrieved 5 re-ranked documents\n",
            "   Text lengths: [332, 331, 384, 337, 375] chars\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "RESULTS\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Precision@5: 0.6000\n",
            "Relevant docs: 3/5\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "DETAILED BREAKDOWN (With Rerank Scores)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "1. INTRODUCTION\n",
            "   ├─ Rerank score: 4.662 (Cross-Encoder)\n",
            "   ├─ Semantic similarity: 0.6457 (Question-Doc)\n",
            "   ├─ Text length: 332 chars\n",
            "   └─ ✗ NOT RELEVANT\n",
            "\n",
            "2. Conclusions:\n",
            "   ├─ Rerank score: 4.627 (Cross-Encoder)\n",
            "   ├─ Semantic similarity: 0.6492 (Question-Doc)\n",
            "   ├─ Text length: 331 chars\n",
            "   └─ ✗ NOT RELEVANT\n",
            "\n",
            "3. Stages of cirrhosis\n",
            "   ├─ Rerank score: 4.514 (Cross-Encoder)\n",
            "   ├─ Semantic similarity: 0.7076 (Question-Doc)\n",
            "   ├─ Text length: 384 chars\n",
            "   └─ ✓ RELEVANT\n",
            "\n",
            "4. Prognosis.\n",
            "   ├─ Rerank score: 2.496 (Cross-Encoder)\n",
            "   ├─ Semantic similarity: 0.7280 (Question-Doc)\n",
            "   ├─ Text length: 337 chars\n",
            "   └─ ✓ RELEVANT\n",
            "\n",
            "5. Conclusions:\n",
            "   ├─ Rerank score: 2.294 (Cross-Encoder)\n",
            "   ├─ Semantic similarity: 0.7115 (Question-Doc)\n",
            "   ├─ Text length: 375 chars\n",
            "   └─ ✓ RELEVANT\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "TIMING BREAKDOWN\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Retrieval: 9.866s\n",
            "Reranking: 0.025s\n",
            "Generation: 17.416s\n",
            "Total: 27.307s\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ASSESSMENT\n",
            "----------------------------------------------------------------------------------------------------\n",
            " FAIR - 60.0% of docs are relevant\n",
            "====================================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TEST: Precision with Re-Ranker\n",
        "test_q = \"How is hepatic fibrosis assessed?\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"PRECISION@K - Re-Ranker Version\")\n",
        "print(\"=\"*100)\n",
        "print(f\"Question: {test_q}\\n\")\n",
        "\n",
        "result = metric_precision_reranker(test_q, k=5, relevance_threshold=0.65)\n",
        "\n",
        "print(f\"\\n\" + \"-\"*100)\n",
        "print(\"RESULTS\")\n",
        "print(\"-\"*100)\n",
        "print(f\"Precision@5: {result['precision_at_k']:.4f}\")\n",
        "print(f\"Relevant docs: {result['relevant_docs']}/{result['total_k']}\")\n",
        "\n",
        "print(f\"\\n\" + \"-\"*100)\n",
        "print(\"DETAILED BREAKDOWN (With Rerank Scores)\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "for i, (heading, rerank_score, semantic_sim, text_len) in enumerate(zip(\n",
        "    result['doc_headings'],\n",
        "    result['rerank_scores'],\n",
        "    result['semantic_similarities'],\n",
        "    result['doc_text_lengths']\n",
        "), 1):\n",
        "    is_relevant = \"✓ RELEVANT\" if semantic_sim > result['threshold'] else \"✗ NOT RELEVANT\"\n",
        "\n",
        "    print(f\"\\n{i}. {heading}\")\n",
        "    print(f\"   ├─ Rerank score: {rerank_score:.3f} (Cross-Encoder)\")\n",
        "    print(f\"   ├─ Semantic similarity: {semantic_sim:.4f} (Question-Doc)\")\n",
        "    print(f\"   ├─ Text length: {text_len} chars\")\n",
        "    print(f\"   └─ {is_relevant}\")\n",
        "\n",
        "print(f\"\\n\" + \"-\"*100)\n",
        "print(\"TIMING BREAKDOWN\")\n",
        "print(\"-\"*100)\n",
        "print(f\"Retrieval: {result['timings'].get('retrieval_s', 0):.3f}s\")\n",
        "print(f\"Reranking: {result['timings'].get('rerank_s', 0):.3f}s\")\n",
        "print(f\"Generation: {result['timings'].get('gen_s', 0):.3f}s\")\n",
        "total_time = (result['timings'].get('retrieval_s', 0) +\n",
        "              result['timings'].get('rerank_s', 0) +\n",
        "              result['timings'].get('gen_s', 0))\n",
        "print(f\"Total: {total_time:.3f}s\")\n",
        "\n",
        "print(f\"\\n\" + \"-\"*100)\n",
        "print(\"ASSESSMENT\")\n",
        "print(\"-\"*100)\n",
        "\n",
        "if result['precision_at_k'] > 0.8:\n",
        "    print(f\" EXCELLENT - {result['precision_at_k']*100:.1f}% of docs are relevant\")\n",
        "elif result['precision_at_k'] > 0.6:\n",
        "    print(f\" GOOD - {result['precision_at_k']*100:.1f}% of docs are relevant\")\n",
        "elif result['precision_at_k'] > 0.4:\n",
        "    print(f\" FAIR - {result['precision_at_k']*100:.1f}% of docs are relevant\")\n",
        "else:\n",
        "    print(f\" POOR - {result['precision_at_k']*100:.1f}% of docs are relevant\")\n",
        "\n",
        "print(\"=\"*100 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHfPEq8VSQ0k",
        "outputId": "7d86a0da-ccb8-4a62-df88-445c223e8401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " METRIC 2 (RECALL@K - MultiQuery) loaded\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# METRIC 2 (MULTIQUERY): RECALL@K - FIXED\n",
        "\n",
        "def metric_2_recall_multiquery(question: str, all_chunks: list, k=5, relevance_threshold=0.65):\n",
        "    \"\"\"\n",
        "    RECALL@K: Out of ALL relevant docs in database, how many did we find in top-K?\n",
        "    Uses multiquery_retrieve_v2 (WITHOUT re-ranker)\n",
        "    \"\"\"\n",
        "\n",
        "    # Get docs from multiquery retriever\n",
        "    retrieved_docs = multiquery_retrieve_v2(question, k_per_query=3, n_related=3, max_total=k)\n",
        "\n",
        "    if not all_chunks:\n",
        "        return {\n",
        "            'recall_at_k': 0.0,\n",
        "            'found': 0,\n",
        "            'total_in_db': 0,\n",
        "            'missed': 0,\n",
        "            'error': 'all_chunks is empty'\n",
        "        }\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return {\n",
        "            'recall_at_k': 0.0,\n",
        "            'found': 0,\n",
        "            'total_in_db': 0,\n",
        "            'missed': 0,\n",
        "            'error': 'No docs retrieved'\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        question_embedding = embedder.embed_query(question)\n",
        "    except Exception as e:\n",
        "        return {'recall_at_k': 0.0, 'found': 0, 'total_in_db': 0, 'missed': 0, 'error': str(e)}\n",
        "\n",
        "    # ===== Find ALL relevant docs in ENTIRE database =====\n",
        "    print(f\"   Scanning all {len(all_chunks)} chunks...\")\n",
        "\n",
        "    all_doc_texts = [chunk.get('text', chunk.get('page_content', chunk.get('content', ''))) for chunk in all_chunks]\n",
        "\n",
        "    try:\n",
        "        all_doc_embeddings = embedder.embed_documents(all_doc_texts)\n",
        "        all_doc_embeddings = np.array(all_doc_embeddings, dtype=np.float32)\n",
        "        all_similarities = cosine_similarity([question_embedding], all_doc_embeddings)[0]\n",
        "\n",
        "        # Count total relevant in entire database\n",
        "        total_relevant_in_db = sum(1 for sim in all_similarities if sim > relevance_threshold)\n",
        "\n",
        "    except Exception as e:\n",
        "        return {'recall_at_k': 0.0, 'found': 0, 'total_in_db': 0, 'missed': 0, 'error': f'Embedding error: {str(e)}'}\n",
        "\n",
        "    if total_relevant_in_db == 0:\n",
        "        return {\n",
        "            'recall_at_k': 0.0,\n",
        "            'found': 0,\n",
        "            'total_in_db': 0,\n",
        "            'missed': 0,\n",
        "            'note': 'No relevant docs in database at threshold'\n",
        "        }\n",
        "\n",
        "    # ===== Count relevant docs in TOP-K retrieved =====\n",
        "    top_k_docs = retrieved_docs[:k]\n",
        "    top_k_texts = [doc.get('text', doc.get('page_content', doc.get('content', ''))) for doc in top_k_docs]\n",
        "\n",
        "    try:\n",
        "        top_k_embeddings = embedder.embed_documents(top_k_texts)\n",
        "        top_k_embeddings = np.array(top_k_embeddings, dtype=np.float32)\n",
        "        top_k_similarities = cosine_similarity([question_embedding], top_k_embeddings)[0]\n",
        "\n",
        "        relevant_in_top_k = sum(1 for sim in top_k_similarities if sim > relevance_threshold)\n",
        "\n",
        "    except Exception as e:\n",
        "        return {'recall_at_k': 0.0, 'found': 0, 'total_in_db': total_relevant_in_db, 'missed': total_relevant_in_db, 'error': f'Top-K embedding error: {str(e)}'}\n",
        "\n",
        "    # RECALL = found / total_relevant\n",
        "    recall = relevant_in_top_k / total_relevant_in_db if total_relevant_in_db > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        'recall_at_k': float(recall),\n",
        "        'found': int(relevant_in_top_k),\n",
        "        'total_in_db': int(total_relevant_in_db),\n",
        "        'missed': int(total_relevant_in_db - relevant_in_top_k),\n",
        "        'k': k,\n",
        "        'threshold': relevance_threshold,\n",
        "        'retrieved_k': len(top_k_docs)\n",
        "    }\n",
        "\n",
        "print(\" METRIC 2 (RECALL@K - MultiQuery) loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud6W69ajf5mg",
        "outputId": "db020885-3180-415e-be7f-353a5f9c27af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What is the definition of hepatic fibrosis?\n",
            "  - 2. How does AASLD diagnose hepatic fibrosis?\n",
            "  - 3. What are the recommended treatment options for hepatic fibrosis according to AASLD?\n",
            "   Scanning all 12788 chunks...\n",
            "\n",
            "======================================================================\n",
            "2️. RECALL@5 (MULTI-QUERY)\n",
            "======================================================================\n",
            "Recall@5:       0.1290\n",
            "Found:          4 / 31 relevant docs\n",
            "Missed:         27 docs\n",
            "Retrieved:      5 docs (k=5)\n",
            "Threshold:      0.65\n",
            "Status:         ⚠ LOW\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# TEST: Recall with Multi-Query Retriever\n",
        "test_q = \"How is hepatic fibrosis assessed according to AASLD?\"\n",
        "r_mq = metric_2_recall_multiquery(test_q, chunks, k=5, relevance_threshold=0.65)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"2️. RECALL@5 (MULTI-QUERY)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check for errors first\n",
        "if 'error' in r_mq:\n",
        "    print(f\" Error: {r_mq['error']}\")\n",
        "    if 'note' in r_mq:\n",
        "        print(f\"   Note: {r_mq['note']}\")\n",
        "else:\n",
        "    print(f\"Recall@5:       {r_mq['recall_at_k']:.4f}\")\n",
        "    print(f\"Found:          {r_mq['found']} / {r_mq['total_in_db']} relevant docs\")\n",
        "    print(f\"Missed:         {r_mq['missed']} docs\")\n",
        "    print(f\"Retrieved:      {r_mq['retrieved_k']} docs (k={r_mq['k']})\")\n",
        "    print(f\"Threshold:      {r_mq['threshold']}\")\n",
        "\n",
        "    if r_mq['recall_at_k'] > 0.6:\n",
        "        print(\"Status:         ✓ GOOD\")\n",
        "    elif r_mq['recall_at_k'] > 0.4:\n",
        "        print(\"Status:         ⚠ FAIR\")\n",
        "    elif r_mq['recall_at_k'] > 0.0:\n",
        "        print(\"Status:         ⚠ LOW\")\n",
        "    else:\n",
        "        print(\"Status:         ✗ ZERO (no relevant docs found in top-k)\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6czakwYW8QEP",
        "outputId": "8546e9e3-0b12-4c9c-9b36-34fcf0cffcea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " METRIC 3 (MRR@K - ReRanker) loaded\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# METRIC 3: MRR@K (MEAN RECIPROCAL RANK) - RERANKER\n",
        "\n",
        "def metric_3_mrr_at_k_reranker(question: str, k=5, relevance_threshold=0.65):\n",
        "    \"\"\"\n",
        "    MRR@K: Mean Reciprocal Rank - Position of FIRST relevant doc in top-K\n",
        "    Uses generate_answer_with_rerank (WITH re-ranker)\n",
        "\n",
        "    MRR = 1 / rank_of_first_relevant_doc\n",
        "    Higher MRR = relevant docs appear earlier in ranking\n",
        "    \"\"\"\n",
        "\n",
        "    # Get answer with re-ranker\n",
        "    result = generate_answer_with_rerank(question, k_per_query=3, n_related=3, max_retrieved=10, rerank_top_k=k)\n",
        "    retrieved_docs = result.get(\"sources\", [])\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return {\n",
        "            'mrr_at_k': 0.0,\n",
        "            'rank_of_first': None,\n",
        "            'first_sim': None,\n",
        "            'error': 'No docs retrieved',\n",
        "            'k': k,\n",
        "            'threshold': relevance_threshold\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        question_embedding = embedder.embed_query(question)\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'mrr_at_k': 0.0,\n",
        "            'rank_of_first': None,\n",
        "            'first_sim': None,\n",
        "            'error': f'Embedding error: {str(e)}',\n",
        "            'k': k,\n",
        "            'threshold': relevance_threshold\n",
        "        }\n",
        "\n",
        "    # ===== Get similarities for top-K =====\n",
        "    top_k_docs = retrieved_docs[:k]\n",
        "    top_k_texts = [doc.get('text', doc.get('heading', doc.get('page_content', ''))) for doc in top_k_docs]\n",
        "\n",
        "    try:\n",
        "        top_k_embeddings = embedder.embed_documents(top_k_texts)\n",
        "        top_k_embeddings = np.array(top_k_embeddings, dtype=np.float32)\n",
        "        similarities = cosine_similarity([question_embedding], top_k_embeddings)[0]\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'mrr_at_k': 0.0,\n",
        "            'rank_of_first': None,\n",
        "            'first_sim': None,\n",
        "            'error': f'Similarity computation error: {str(e)}',\n",
        "            'k': k,\n",
        "            'threshold': relevance_threshold\n",
        "        }\n",
        "\n",
        "    # ===== Find rank of first relevant doc =====\n",
        "    for rank, sim in enumerate(similarities, start=1):\n",
        "        if sim > relevance_threshold:\n",
        "            mrr = 1.0 / rank\n",
        "            return {\n",
        "                'mrr_at_k': float(mrr),\n",
        "                'rank_of_first': int(rank),\n",
        "                'first_sim': float(sim),\n",
        "                'k': k,\n",
        "                'threshold': relevance_threshold,\n",
        "                'note': 'Relevant doc found'\n",
        "            }\n",
        "\n",
        "    # No relevant doc found in top-K\n",
        "    return {\n",
        "        'mrr_at_k': 0.0,\n",
        "        'rank_of_first': None,\n",
        "        'first_sim': None,\n",
        "        'note': 'No relevant doc in top-K',\n",
        "        'k': k,\n",
        "        'threshold': relevance_threshold\n",
        "    }\n",
        "\n",
        "print(\" METRIC 3 (MRR@K - ReRanker) loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM3uSTz89rn3",
        "outputId": "0f2d30d3-6edc-41e8-d79c-cec3eca6ecc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[STEP 1] Multi-Query Retrieval...\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. How is liver fibrosis measured?\n",
            "  - 2. What are the different stages of hepatic fibrosis according to the AASLD guidelines?\n",
            "  - 3. How is liver fibrosis progression evaluated in patients with AASLD liver disease guidelines?\n",
            "  Retrieved 8 candidates in 5.670s\n",
            "\n",
            "[STEP 2] Re-Ranking with Cross-Encoder...\n",
            " Re-ranked 8 docs → top 5\n",
            "     1. Supplemental Digital Content... (score: 5.610)\n",
            "     2. INTRODUCTION... (score: 4.743)\n",
            "     3. Conclusions:... (score: 3.750)\n",
            "     4. KLF9 drives intermittent hypoxia-induced NAFLD by ... (score: 3.189)\n",
            "     5. US-based elastography... (score: 1.432)\n",
            "  Re-ranking completed in 0.014s\n",
            "\n",
            "[STEP 3] Generating Answer...\n",
            "  Answer generated in 9.410s\n",
            "\n",
            "======================================================================\n",
            "3️.  MRR@5 (RE-RANKER)\n",
            "======================================================================\n",
            "MRR@5:                  1.0000\n",
            "First relevant doc at:  Rank 1\n",
            "Similarity score:       0.7162\n",
            "Threshold:              0.65\n",
            "Status:                 ✓ PERFECT (relevant at rank 1)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# TEST: MRR with Re-Ranker\n",
        "test_q = \"How is hepatic fibrosis assessed according to AASLD?\"\n",
        "m_rr = metric_3_mrr_at_k_reranker(test_q, k=5, relevance_threshold=0.65)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"3️.  MRR@5 (RE-RANKER)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if 'error' in m_rr:\n",
        "    print(f\" Error: {m_rr['error']}\")\n",
        "else:\n",
        "    print(f\"MRR@5:                  {m_rr['mrr_at_k']:.4f}\")\n",
        "\n",
        "    if m_rr['rank_of_first'] is not None:\n",
        "        print(f\"First relevant doc at:  Rank {m_rr['rank_of_first']}\")\n",
        "        print(f\"Similarity score:       {m_rr['first_sim']:.4f}\")\n",
        "        print(f\"Threshold:              {m_rr['threshold']}\")\n",
        "\n",
        "        if m_rr['rank_of_first'] == 1:\n",
        "            print(\"Status:                 ✓ PERFECT (relevant at rank 1)\")\n",
        "        elif m_rr['rank_of_first'] <= 3:\n",
        "            print(\"Status:                 ✓ GOOD (relevant in top-3)\")\n",
        "        elif m_rr['rank_of_first'] <= 5:\n",
        "            print(\"Status:                 ⚠ FAIR (relevant but not top-3)\")\n",
        "        else:\n",
        "            print(\"Status:                 ⚠ LOW\")\n",
        "    else:\n",
        "        print(\"First relevant doc:     NOT FOUND\")\n",
        "        print(f\"Status:                 ✗ {m_rr.get('note', 'No relevant docs in top-K')}\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftAGIhzx_gyo",
        "outputId": "82e6fd37-9385-41d9-9065-cd9158186633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ METRIC 4 (NDCG@K - MultiQuery) loaded\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# METRIC 4: NDCG@K - MULTIQUERY\n",
        "# ======================================\n",
        "def metric_4_ndcg_multiquery(question: str, k=5):\n",
        "    \"\"\"\n",
        "    NDCG@K: Normalized Discounted Cumulative Gain - Ranking quality\n",
        "    Uses multiquery_retrieve_v2 (WITHOUT re-ranker)\n",
        "\n",
        "    NDCG ranges 0-1: 1.0 = perfect ranking, 0.0 = worst ranking\n",
        "    \"\"\"\n",
        "\n",
        "    # Get docs from multiquery retriever\n",
        "    retrieved_docs = multiquery_retrieve_v2(question, k_per_query=3, n_related=3, max_total=k)\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return {\n",
        "            'ndcg_at_k': 0.0,\n",
        "            'dcg': 0.0,\n",
        "            'idcg': 0.0,\n",
        "            'quality': 'Error',\n",
        "            'error': 'No docs retrieved',\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    top_k_docs = retrieved_docs[:k]\n",
        "    top_k_texts = [doc.get('text', doc.get('page_content', doc.get('content', ''))) for doc in top_k_docs]\n",
        "\n",
        "    try:\n",
        "        # Embed question and documents\n",
        "        question_embedding = embedder.embed_query(question)\n",
        "        doc_embeddings = embedder.embed_documents(top_k_texts)\n",
        "        doc_embeddings = np.array(doc_embeddings, dtype=np.float32)\n",
        "\n",
        "        # Get similarity scores as relevance gains\n",
        "        relevances = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'ndcg_at_k': 0.0,\n",
        "            'dcg': 0.0,\n",
        "            'idcg': 0.0,\n",
        "            'quality': 'Error',\n",
        "            'error': f'Embedding error: {str(e)}',\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    # ===== Calculate DCG (Discounted Cumulative Gain) =====\n",
        "    dcg = 0.0\n",
        "    for rank, relevance in enumerate(relevances, start=1):\n",
        "        dcg += relevance / np.log2(rank + 1)\n",
        "\n",
        "    # ===== Calculate IDCG (Ideal DCG - perfect ranking) =====\n",
        "    ideal_relevances = np.sort(relevances)[::-1]  # Sort descending\n",
        "    idcg = 0.0\n",
        "    for rank, relevance in enumerate(ideal_relevances, start=1):\n",
        "        idcg += relevance / np.log2(rank + 1)\n",
        "\n",
        "    # ===== Normalize =====\n",
        "    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "    # Determine quality rating\n",
        "    if ndcg > 0.9:\n",
        "        quality = \"Excellent\"\n",
        "    elif ndcg > 0.7:\n",
        "        quality = \"Good\"\n",
        "    elif ndcg > 0.5:\n",
        "        quality = \"Fair\"\n",
        "    else:\n",
        "        quality = \"Poor\"\n",
        "\n",
        "    return {\n",
        "        'ndcg_at_k': float(ndcg),\n",
        "        'dcg': float(dcg),\n",
        "        'idcg': float(idcg),\n",
        "        'quality': quality,\n",
        "        'relevances': [float(r) for r in relevances],\n",
        "        'ideal_relevances': [float(r) for r in ideal_relevances],\n",
        "        'k': k\n",
        "    }\n",
        "\n",
        "print(\"✓ METRIC 4 (NDCG@K - MultiQuery) loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g6-AoWk_7ZK",
        "outputId": "7a6aeabc-f3e9-446c-d954-6688f639674b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What is the role of imaging techniques in assessing hepatic fibrosis according to AASLD guidelines?\n",
            "  - 2. How does the AASLD measure the severity of hepatic fibrosis?\n",
            "  - 3. Are there any specific biomarkers used by the AASLD to diagnose hepatic fibrosis?\n",
            "\n",
            "======================================================================\n",
            "4️. NDCG@5 (MULTI-QUERY)\n",
            "======================================================================\n",
            "NDCG@5:                 0.9853\n",
            "Quality:                Excellent\n",
            "DCG:                    1.8834\n",
            "IDCG (Ideal):           1.9114\n",
            "\n",
            "└─ Relevance scores (current ranking):\n",
            "   Rank 1: 0.6438\n",
            "   Rank 2: 0.6145\n",
            "   Rank 3: 0.6145\n",
            "   Rank 4: 0.6680\n",
            "   Rank 5: 0.6644\n",
            "\n",
            "Status:                 ✓ EXCELLENT ranking quality\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# TEST: NDCG with Multi-Query Retriever\n",
        "test_q = \"How is hepatic fibrosis assessed according to AASLD?\"\n",
        "n_mq = metric_4_ndcg_multiquery(test_q, k=5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"4️. NDCG@5 (MULTI-QUERY)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if 'error' in n_mq:\n",
        "    print(f\" Error: {n_mq['error']}\")\n",
        "else:\n",
        "    print(f\"NDCG@5:                 {n_mq['ndcg_at_k']:.4f}\")\n",
        "    print(f\"Quality:                {n_mq['quality']}\")\n",
        "    print(f\"DCG:                    {n_mq['dcg']:.4f}\")\n",
        "    print(f\"IDCG (Ideal):           {n_mq['idcg']:.4f}\")\n",
        "\n",
        "    print(f\"\\n└─ Relevance scores (current ranking):\")\n",
        "    for rank, rel in enumerate(n_mq['relevances'], 1):\n",
        "        print(f\"   Rank {rank}: {rel:.4f}\")\n",
        "\n",
        "    if n_mq['ndcg_at_k'] > 0.9:\n",
        "        print(\"\\nStatus:                 ✓ EXCELLENT ranking quality\")\n",
        "    elif n_mq['ndcg_at_k'] > 0.7:\n",
        "        print(\"\\nStatus:                 ✓ GOOD ranking quality\")\n",
        "    elif n_mq['ndcg_at_k'] > 0.5:\n",
        "        print(\"\\nStatus:                 ⚠ FAIR ranking quality\")\n",
        "    else:\n",
        "        print(\"\\nStatus:                 ✗ POOR ranking quality\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaeVyQkv_9ox",
        "outputId": "21f8392f-f75a-4d7b-b6c9-dc73f5d2c006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ METRIC 4 (NDCG@K - ReRanker) loaded\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# METRIC 4: NDCG@K - RERANKER\n",
        "# ======================================\n",
        "def metric_4_ndcg_reranker(question: str, k=5):\n",
        "    \"\"\"\n",
        "    NDCG@K: Normalized Discounted Cumulative Gain - Ranking quality\n",
        "    Uses generate_answer_with_rerank (WITH re-ranker)\n",
        "    \"\"\"\n",
        "\n",
        "    # Get answer with re-ranker\n",
        "    result = generate_answer_with_rerank(question, k_per_query=3, n_related=3, max_retrieved=10, rerank_top_k=k)\n",
        "    retrieved_docs = result.get(\"sources\", [])\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return {\n",
        "            'ndcg_at_k': 0.0,\n",
        "            'dcg': 0.0,\n",
        "            'idcg': 0.0,\n",
        "            'quality': 'Error',\n",
        "            'error': 'No docs retrieved',\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    top_k_docs = retrieved_docs[:k]\n",
        "    top_k_texts = [doc.get('text', doc.get('heading', doc.get('page_content', ''))) for doc in top_k_docs]\n",
        "\n",
        "    try:\n",
        "        # Embed question and documents\n",
        "        question_embedding = embedder.embed_query(question)\n",
        "        doc_embeddings = embedder.embed_documents(top_k_texts)\n",
        "        doc_embeddings = np.array(doc_embeddings, dtype=np.float32)\n",
        "\n",
        "        # Get similarity scores as relevance gains\n",
        "        relevances = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'ndcg_at_k': 0.0,\n",
        "            'dcg': 0.0,\n",
        "            'idcg': 0.0,\n",
        "            'quality': 'Error',\n",
        "            'error': f'Embedding error: {str(e)}',\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    # ===== Calculate DCG (Discounted Cumulative Gain) =====\n",
        "    dcg = 0.0\n",
        "    for rank, relevance in enumerate(relevances, start=1):\n",
        "        dcg += relevance / np.log2(rank + 1)\n",
        "\n",
        "    # ===== Calculate IDCG (Ideal DCG - perfect ranking) =====\n",
        "    ideal_relevances = np.sort(relevances)[::-1]  # Sort descending\n",
        "    idcg = 0.0\n",
        "    for rank, relevance in enumerate(ideal_relevances, start=1):\n",
        "        idcg += relevance / np.log2(rank + 1)\n",
        "\n",
        "    # ===== Normalize =====\n",
        "    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "    # Determine quality rating\n",
        "    if ndcg > 0.9:\n",
        "        quality = \"Excellent\"\n",
        "    elif ndcg > 0.7:\n",
        "        quality = \"Good\"\n",
        "    elif ndcg > 0.5:\n",
        "        quality = \"Fair\"\n",
        "    else:\n",
        "        quality = \"Poor\"\n",
        "\n",
        "    return {\n",
        "        'ndcg_at_k': float(ndcg),\n",
        "        'dcg': float(dcg),\n",
        "        'idcg': float(idcg),\n",
        "        'quality': quality,\n",
        "        'relevances': [float(r) for r in relevances],\n",
        "        'ideal_relevances': [float(r) for r in ideal_relevances],\n",
        "        'k': k\n",
        "    }\n",
        "\n",
        "print(\"✓ METRIC 4 (NDCG@K - ReRanker) loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfTwN7fbAhq9",
        "outputId": "88c52b68-300f-435c-fa82-dd7e0a227439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[STEP 1] Multi-Query Retrieval...\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What are the different methods used to assess hepatic fibrosis?\n",
            "  - 2. How do these methods compare in terms of accuracy and reliability?\n",
            "  - 3. Are there any limitations or challenges associated with using these methods?\n",
            "  Retrieved 10 candidates in 1.538s\n",
            "\n",
            "[STEP 2] Re-Ranking with Cross-Encoder...\n",
            " Re-ranked 10 docs → top 5\n",
            "     1. Supplemental Digital Content... (score: 5.610)\n",
            "     2. INTRODUCTION... (score: 4.743)\n",
            "     3. Conclusions:... (score: 3.750)\n",
            "     4. Conclusions:... (score: 1.721)\n",
            "     5. INTRODUCTION... (score: 1.526)\n",
            "  Re-ranking completed in 0.017s\n",
            "\n",
            "[STEP 3] Generating Answer...\n",
            "  Answer generated in 9.638s\n",
            "\n",
            "======================================================================\n",
            "4️.  NDCG@5 (RE-RANKER)\n",
            "======================================================================\n",
            "NDCG@5:                 0.9998\n",
            "Quality:                Excellent\n",
            "DCG:                    2.0427\n",
            "IDCG (Ideal):           2.0431\n",
            "\n",
            "└─ Relevance scores (current ranking):\n",
            "   Rank 1: 0.7162\n",
            "   Rank 2: 0.7097\n",
            "   Rank 3: 0.7128\n",
            "   Rank 4: 0.6416\n",
            "   Rank 5: 0.6358\n",
            "\n",
            "Status:                 ✓ EXCELLENT ranking quality\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "COMPARISON: MultiQuery vs ReRanker (NDCG@5)\n",
            "----------------------------------------------------------------------\n",
            "MultiQuery NDCG@5:      0.9853 (Excellent)\n",
            "ReRanker NDCG@5:        0.9998 (Excellent)\n",
            "Improvement:            1.5%\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# TEST: NDCG with Re-Ranker\n",
        "test_q = \"How is hepatic fibrosis assessed according to AASLD?\"\n",
        "n_rr = metric_4_ndcg_reranker(test_q, k=5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"4️.  NDCG@5 (RE-RANKER)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if 'error' in n_rr:\n",
        "    print(f\" Error: {n_rr['error']}\")\n",
        "else:\n",
        "    print(f\"NDCG@5:                 {n_rr['ndcg_at_k']:.4f}\")\n",
        "    print(f\"Quality:                {n_rr['quality']}\")\n",
        "    print(f\"DCG:                    {n_rr['dcg']:.4f}\")\n",
        "    print(f\"IDCG (Ideal):           {n_rr['idcg']:.4f}\")\n",
        "\n",
        "    print(f\"\\n└─ Relevance scores (current ranking):\")\n",
        "    for rank, rel in enumerate(n_rr['relevances'], 1):\n",
        "        print(f\"   Rank {rank}: {rel:.4f}\")\n",
        "\n",
        "    if n_rr['ndcg_at_k'] > 0.9:\n",
        "        print(\"\\nStatus:                 ✓ EXCELLENT ranking quality\")\n",
        "    elif n_rr['ndcg_at_k'] > 0.7:\n",
        "        print(\"\\nStatus:                 ✓ GOOD ranking quality\")\n",
        "    elif n_rr['ndcg_at_k'] > 0.5:\n",
        "        print(\"\\nStatus:                 ⚠ FAIR ranking quality\")\n",
        "    else:\n",
        "        print(\"\\nStatus:                 ✗ POOR ranking quality\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"COMPARISON: MultiQuery vs ReRanker (NDCG@5)\")\n",
        "print(\"-\"*70)\n",
        "print(f\"MultiQuery NDCG@5:      {n_mq['ndcg_at_k']:.4f} ({n_mq['quality']})\")\n",
        "print(f\"ReRanker NDCG@5:        {n_rr['ndcg_at_k']:.4f} ({n_rr['quality']})\")\n",
        "\n",
        "if n_mq['ndcg_at_k'] > 0:\n",
        "    improvement_pct = ((n_rr['ndcg_at_k'] - n_mq['ndcg_at_k']) / n_mq['ndcg_at_k']) * 100\n",
        "    print(f\"Improvement:            {improvement_pct:.1f}%\")\n",
        "else:\n",
        "    print(f\"Improvement:            ReRanker outperformed (MultiQuery was 0)\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbaem9Y82ejE",
        "outputId": "501b85ce-e5ec-40ca-946e-6fd2e555fa92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        " !pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiQs_ydLAjyu",
        "outputId": "3648c7d6-8b52-453f-e17c-05d028ff4a50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ METRIC 5 (ROUGE - MultiQuery) loaded - OPTIMIZED\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# METRIC 5: ROUGE - MULTIQUERY (OPTIMIZED)\n",
        "# ======================================\n",
        "def metric_5_rouge_multiquery(generated_answer: str, sources: list, question: str = \"\"):\n",
        "    \"\"\"\n",
        "    ROUGE: Answer quality assessment using multi-query retrieval\n",
        "\n",
        "    Args:\n",
        "        generated_answer: The answer text (already generated)\n",
        "        sources: Retrieved source documents\n",
        "        question: Original question (fallback for reference)\n",
        "    \"\"\"\n",
        "\n",
        "    if not generated_answer:\n",
        "        return {\n",
        "            'rouge1': 0.0,\n",
        "            'rouge2': 0.0,\n",
        "            'rougeL': 0.0,\n",
        "            'avg': 0.0,\n",
        "            'error': 'Missing answer',\n",
        "            'answer_length': 0\n",
        "        }\n",
        "\n",
        "    # Build reference from top-2 chunks\n",
        "    ref_texts = []\n",
        "    for doc in sources[:2]:\n",
        "        text = doc.get('text', doc.get('page_content', ''))\n",
        "        if text:\n",
        "            ref_texts.append(text[:300])  # Use first 300 chars\n",
        "    reference_answer = ' '.join(ref_texts) if ref_texts else question\n",
        "\n",
        "    if not reference_answer:\n",
        "        return {\n",
        "            'rouge1': 0.0,\n",
        "            'rouge2': 0.0,\n",
        "            'rougeL': 0.0,\n",
        "            'avg': 0.0,\n",
        "            'error': 'No reference text',\n",
        "            'answer_length': len(generated_answer)\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        from rouge_score import rouge_scorer\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        scores = scorer.score(reference_answer, generated_answer)\n",
        "\n",
        "        return {\n",
        "            'rouge1': float(scores['rouge1'].fmeasure),\n",
        "            'rouge2': float(scores['rouge2'].fmeasure),\n",
        "            'rougeL': float(scores['rougeL'].fmeasure),\n",
        "            'avg': float((scores['rouge1'].fmeasure + scores['rouge2'].fmeasure + scores['rougeL'].fmeasure) / 3),\n",
        "            'answer_length': len(generated_answer)\n",
        "        }\n",
        "\n",
        "    except ImportError:\n",
        "        return {\n",
        "            'rouge1': 0.0,\n",
        "            'rouge2': 0.0,\n",
        "            'rougeL': 0.0,\n",
        "            'avg': 0.0,\n",
        "            'error': 'rouge_score not installed',\n",
        "            'answer_length': len(generated_answer)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'rouge1': 0.0,\n",
        "            'rouge2': 0.0,\n",
        "            'rougeL': 0.0,\n",
        "            'avg': 0.0,\n",
        "            'error': f'ROUGE error: {str(e)}',\n",
        "            'answer_length': len(generated_answer)\n",
        "        }\n",
        "\n",
        "print(\"✓ METRIC 5 (ROUGE - MultiQuery) loaded - OPTIMIZED\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zETYk3HnBVuH",
        "outputId": "b871dddc-1762-4355-d664-20111062ab40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "3️.  ROUGE (MULTI-QUERY)\n",
            "====================================================================================================\n",
            "Question: How is hepatic fibrosis assessed?\n",
            "\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What are the diagnostic tests used to assess hepatic fibrosis?\n",
            "  - 2. How are the results of these diagnostic tests interpreted to determine the extent of fibrosis?\n",
            "  - 3. What are the potential treatment options for individuals diagnosed with hepatic fibrosis?\n",
            "ROUGE-1 (unigram):      0.3607\n",
            "ROUGE-2 (bigram):       0.1405\n",
            "ROUGE-L (sequence):     0.1967\n",
            "Average ROUGE:          0.2326\n",
            "Answer length:          1174 chars\n",
            "\n",
            "Status:                 ⚠ FAIR (Some overlap)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "REFERENCE (from top-2 chunks)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Chunk 1]:\n",
            ". The role of serum‐based biomarker panels for the assessment of hepatic fibrosis remains unestablished in autoimmune hepatitis.4 Our objectives for this systematic review were to address 3 key questi...\n",
            "\n",
            "[Chunk 2]:\n",
            ". The role of serum‐based biomarker panels for the assessment of hepatic fibrosis remains unestablished in autoimmune hepatitis.4Our objectives for this systematic review were to address 3 key questio...\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "GENERATED ANSWER\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Hepatic fibrosis is assessed using a combination of blood-based biomarkers and liver biopsy as the reference standard. The accuracy of blood-based biomarkers for staging fibrosis is still unestablished in autoimmune hepatitis. However, they can provide valuable information about disease severity and prognosis. Liver biopsy is the gold standard for assessing fibrosis, as it allows for direct visualization of liver tissue and measurement of fibrosis score.\n",
            "\n",
            "[Section: Prognosis. | Source: dedbaae96e4582222667a55c1a3e63917ef3d786e15fc348409c48687724fd08_cleaned.json]\n",
            "    \n",
            "    Question: How does fibrosis assessment impact prognosis?\n",
            "\n",
            "    Answer:\n",
            "    Fibrosis assessment plays a crucial role in determining prognosis, particularly in the case of hepatitis C. Fibrosis assessment helps predict the emergence of complications of portal hypertension and liver-related morbidity and mortality. Elevated fibrosis scores indicate a higher risk of developing cirrhosis and its associated complications. Therefore, accurate and timely assessment of fibrosis is essential for guiding treatment decisions and monitoring disease progression.\n",
            "\n",
            "[Section: US-based elastography | Source\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# TEST: ROUGE with Multi-Query Retriever\n",
        "test_q = \"How is hepatic fibrosis assessed?\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"3️.  ROUGE (MULTI-QUERY)\")\n",
        "print(\"=\"*100)\n",
        "print(f\"Question: {test_q}\\n\")\n",
        "\n",
        "# Generate answer once\n",
        "result_mq = generate_answer_multiquery_v2(test_q, k_per_query=3, n_related=3, max_total=5)\n",
        "answer_mq = result_mq.get(\"answer\", \"\")\n",
        "sources_mq = result_mq.get(\"sources\", [])\n",
        "\n",
        "# Calculate ROUGE\n",
        "ro_mq = metric_5_rouge_multiquery(answer_mq, sources_mq, test_q)\n",
        "\n",
        "print(f\"ROUGE-1 (unigram):      {ro_mq.get('rouge1', 0.0):.4f}\")\n",
        "print(f\"ROUGE-2 (bigram):       {ro_mq.get('rouge2', 0.0):.4f}\")\n",
        "print(f\"ROUGE-L (sequence):     {ro_mq.get('rougeL', 0.0):.4f}\")\n",
        "print(f\"Average ROUGE:          {ro_mq.get('avg', 0.0):.4f}\")\n",
        "print(f\"Answer length:          {ro_mq.get('answer_length', 0)} chars\")\n",
        "\n",
        "if ro_mq.get('error'):\n",
        "    print(f\"\\n⚠️  Note: {ro_mq['error']}\")\n",
        "\n",
        "# Show quality assessment\n",
        "avg_score = ro_mq.get('avg', 0.0)\n",
        "if avg_score > 0.5:\n",
        "    print(\"\\nStatus:                 ✓ GOOD (High overlap with reference)\")\n",
        "elif avg_score > 0.2:\n",
        "    print(\"\\nStatus:                 ⚠ FAIR (Some overlap)\")\n",
        "else:\n",
        "    print(\"\\nStatus:                 ✗ LOW (Minimal overlap - likely hallucination)\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*100)\n",
        "print(\"REFERENCE (from top-2 chunks)\")\n",
        "print(\"-\"*100)\n",
        "ref_texts = []\n",
        "for i, doc in enumerate(sources_mq[:2], 1):\n",
        "    text = doc.get('text', doc.get('page_content', ''))[:200]\n",
        "    print(f\"\\n[Chunk {i}]:\\n{text}...\")\n",
        "    ref_texts.append(text)\n",
        "\n",
        "print(\"\\n\" + \"-\"*100)\n",
        "print(\"GENERATED ANSWER\")\n",
        "print(\"-\"*100)\n",
        "print(f\"\\n{answer_mq}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P7hq-EJCzFp",
        "outputId": "77fffaff-6ca3-42b7-8bcf-4111e3ceab7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " METRIC 5 (ROUGE - ReRanker) loaded - OPTIMIZED\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# METRIC 5: ROUGE - RERANKER (OPTIMIZED)\n",
        "# ======================================\n",
        "def metric_5_rouge_reranker(generated_answer: str, sources: list, question: str = \"\"):\n",
        "    \"\"\"\n",
        "    ROUGE: Answer quality assessment using re-ranker\n",
        "\n",
        "    Args:\n",
        "        generated_answer: The answer text (already generated)\n",
        "        sources: Re-ranked source documents\n",
        "        question: Original question (fallback for reference)\n",
        "    \"\"\"\n",
        "\n",
        "    if not generated_answer:\n",
        "        return {\n",
        "            'rouge1': 0.0,\n",
        "            'rouge2': 0.0,\n",
        "            'rougeL': 0.0,\n",
        "            'avg': 0.0,\n",
        "            'error': 'Missing answer',\n",
        "            'answer_length': 0\n",
        "        }\n",
        "\n",
        "    # Build reference from top-2 chunks\n",
        "    ref_texts = []\n",
        "    for doc in sources[:2]:\n",
        "        text = doc.get('text', doc.get('page_content', ''))\n",
        "        if text:\n",
        "            ref_texts.append(text[:300])  # Use first 300 chars\n",
        "    reference_answer = ' '.join(ref_texts) if ref_texts else question\n",
        "\n",
        "    if not reference_answer:\n",
        "        return {\n",
        "            'rouge1': 0.0,\n",
        "            'rouge2': 0.0,\n",
        "            'rougeL': 0.0,\n",
        "            'avg': 0.0,\n",
        "            'error': 'No reference text',\n",
        "            'answer_length': len(generated_answer)\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        from rouge_score import rouge_scorer\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        scores = scorer.score(reference_answer, generated_answer)\n",
        "\n",
        "        return {\n",
        "            'rouge1': float(scores['rouge1'].fmeasure),\n",
        "            'rouge2': float(scores['rouge2'].fmeasure),\n",
        "            'rougeL': float(scores['rougeL'].fmeasure),\n",
        "            'avg': float((scores['rouge1'].fmeasure + scores['rouge2'].fmeasure + scores['rougeL'].fmeasure) / 3),\n",
        "            'answer_length': len(generated_answer)\n",
        "        }\n",
        "\n",
        "    except ImportError:\n",
        "        return {\n",
        "            'rouge1': 0.0,\n",
        "            'rouge2': 0.0,\n",
        "            'rougeL': 0.0,\n",
        "            'avg': 0.0,\n",
        "            'error': 'rouge_score not installed',\n",
        "            'answer_length': len(generated_answer)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'rouge1': 0.0,\n",
        "            'rouge2': 0.0,\n",
        "            'rougeL': 0.0,\n",
        "            'avg': 0.0,\n",
        "            'error': f'ROUGE error: {str(e)}',\n",
        "            'answer_length': len(generated_answer)\n",
        "        }\n",
        "\n",
        "print(\" METRIC 5 (ROUGE - ReRanker) loaded - OPTIMIZED\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1cvna8BC6Uw",
        "outputId": "824d26de-62b4-4368-dbea-a442e9168eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====================================================================================================\n",
            "3️. ROUGE (RE-RANKER)\n",
            "====================================================================================================\n",
            "Question: How is hepatic fibrosis assessed?\n",
            "\n",
            "\n",
            "[STEP 1] Multi-Query Retrieval...\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What are the common diagnostic tests used for assessing hepatic fibrosis?\n",
            "  - 2. How does liver biopsy contribute to the assessment of fibrosis in AASLD liver disease guidelines?\n",
            "  - 3. What are the potential limitations or risks associated with liver biopsy as a diagnostic tool for fibrosis?\n",
            "  Retrieved 9 candidates in 10.968s\n",
            "\n",
            "[STEP 2] Re-Ranking with Cross-Encoder...\n",
            " Re-ranked 9 docs → top 5\n",
            "     1. INTRODUCTION... (score: 4.662)\n",
            "     2. Conclusions:... (score: 4.627)\n",
            "     3. Conclusions:... (score: 4.168)\n",
            "     4. Prognosis.... (score: 2.496)\n",
            "     5. Conclusions:... (score: 2.294)\n",
            "  Re-ranking completed in 0.018s\n",
            "\n",
            "[STEP 3] Generating Answer...\n",
            "  Answer generated in 17.080s\n",
            "ROUGE-1 (unigram):      0.4565\n",
            "ROUGE-2 (bigram):       0.3431\n",
            "ROUGE-L (sequence):     0.4275\n",
            "Average ROUGE:          0.4090\n",
            "Answer length:          1235 chars\n",
            "\n",
            "Status:                 ⚠ FAIR (Some overlap)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "REFERENCE (from top-2 chunks)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Chunk 1]:\n",
            ". The role of serum‐based biomarker panels for the assessment of hepatic fibrosis remains unestablished in autoimmune hepatitis.4 Our objectives for this systematic review were to address 3 key questi...\n",
            "\n",
            "[Chunk 2]:\n",
            ". The role of serum‐based biomarker panels for the assessment of hepatic fibrosis remains unestablished in autoimmune hepatitis.4Our objectives for this systematic review were to address 3 key questio...\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "GENERATED ANSWER\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "The role of serum‐based biomarker panels for the assessment of hepatic fibrosis remains unestablished in autoimmune hepatitis.4Our objectives for this systematic review were to address 3 key questions (Table 1) as related to the accuracy of blood-based biomarkers for staging fibrosis, with liver biopsy as the reference standard\n",
            "    Accurate assessment of the degree of liver fibrosis and steatosis is essential in predicting prognosis and making treatment recommendations in patients with CLD. Although liver biopsy has long been the reference standard for assessing fibrosis and steatosis, it is costly, invasive, and carries a small, but important, risk of complications\n",
            "    A further important use of liver biopsy is in assessing disease severity, notably fibrosis, which, as a precursor to cirrhosis, may predict the emergence of complications of portal hypertension and also liver‐related morbidity and mortality\n",
            "    The diagnostic utility of fibrosis-4 or nonalcoholic fatty liver disease fibrosis score combined with liver stiffness measurement by fibroscan in assessment of advanced liver fibrosis: a biopsy-proven nonalcoholic fatty liver disease study. Eur J Gastroenterol Hepatol. 2020;32:642–649. Google Scholar258. Tseng\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "COMPARISON: MultiQuery vs ReRanker (ROUGE)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MultiQuery ROUGE Avg:   0.2326\n",
            "ReRanker ROUGE Avg:     0.4090\n",
            "Improvement:            +75.8%\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# TEST: ROUGE with Re-Ranker\n",
        "test_q = \"How is hepatic fibrosis assessed?\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"3️. ROUGE (RE-RANKER)\")\n",
        "print(\"=\"*100)\n",
        "print(f\"Question: {test_q}\\n\")\n",
        "\n",
        "# Generate answer once\n",
        "result_rr = generate_answer_with_rerank(test_q, k_per_query=3, n_related=3, max_retrieved=10, rerank_top_k=5)\n",
        "answer_rr = result_rr.get(\"answer\", \"\")\n",
        "sources_rr = result_rr.get(\"sources\", [])\n",
        "\n",
        "# Calculate ROUGE\n",
        "ro_rr = metric_5_rouge_reranker(answer_rr, sources_rr, test_q)\n",
        "\n",
        "print(f\"ROUGE-1 (unigram):      {ro_rr.get('rouge1', 0.0):.4f}\")\n",
        "print(f\"ROUGE-2 (bigram):       {ro_rr.get('rouge2', 0.0):.4f}\")\n",
        "print(f\"ROUGE-L (sequence):     {ro_rr.get('rougeL', 0.0):.4f}\")\n",
        "print(f\"Average ROUGE:          {ro_rr.get('avg', 0.0):.4f}\")\n",
        "print(f\"Answer length:          {ro_rr.get('answer_length', 0)} chars\")\n",
        "\n",
        "if ro_rr.get('error'):\n",
        "    print(f\"\\n⚠️  Note: {ro_rr['error']}\")\n",
        "\n",
        "# Show quality assessment\n",
        "avg_score = ro_rr.get('avg', 0.0)\n",
        "if avg_score > 0.5:\n",
        "    print(\"\\nStatus:                 ✓ GOOD (High overlap with reference)\")\n",
        "elif avg_score > 0.2:\n",
        "    print(\"\\nStatus:                 ⚠ FAIR (Some overlap)\")\n",
        "else:\n",
        "    print(\"\\nStatus:                 ✗ LOW (Minimal overlap - likely hallucination)\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*100)\n",
        "print(\"REFERENCE (from top-2 chunks)\")\n",
        "print(\"-\"*100)\n",
        "ref_texts = []\n",
        "for i, doc in enumerate(sources_rr[:2], 1):\n",
        "    text = doc.get('text', doc.get('page_content', ''))[:200]\n",
        "    print(f\"\\n[Chunk {i}]:\\n{text}...\")\n",
        "    ref_texts.append(text)\n",
        "\n",
        "print(\"\\n\" + \"-\"*100)\n",
        "print(\"GENERATED ANSWER\")\n",
        "print(\"-\"*100)\n",
        "print(f\"\\n{answer_rr}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*100)\n",
        "print(\"COMPARISON: MultiQuery vs ReRanker (ROUGE)\")\n",
        "print(\"-\"*100)\n",
        "print(f\"MultiQuery ROUGE Avg:   {ro_mq.get('avg', 0.0):.4f}\")\n",
        "print(f\"ReRanker ROUGE Avg:     {ro_rr.get('avg', 0.0):.4f}\")\n",
        "\n",
        "if ro_mq.get('avg', 0.0) > 0:\n",
        "    improvement = ((ro_rr.get('avg', 0.0) - ro_mq.get('avg', 0.0)) / ro_mq.get('avg', 0.0)) * 100\n",
        "    print(f\"Improvement:            {improvement:+.1f}%\")\n",
        "else:\n",
        "    print(f\"Improvement:            Cannot compute (MultiQuery is 0)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "zUhmjZS3fR2P"
      },
      "outputs": [],
      "source": [
        "# evaluation for 5 questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfgAtNyOC8iE",
        "outputId": "822d2f91-9c90-4414-89a7-0f5f14821e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================================================================================================\n",
            "📊 FINAL BATCH EVALUATION: 5 BALANCED QUESTIONS × 7 METRICS\n",
            "==================================================================================================================================\n",
            "Columns: Precision (MQ/RR) | NDCG (MQ/RR) | ROUGE (MQ/RR) | ROUGE Improvement %\n",
            "==================================================================================================================================\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Q1: How is Wilson disease diagnosed and what are the diagnostic tests?\n",
            "──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - What are the symptoms of Wilson disease?\n",
            "  - How is Wilson disease treated?\n",
            "  - What is the prognosis for Wilson disease?\n",
            "\n",
            "[STEP 1] Multi-Query Retrieval...\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What is the process for diagnosing Wilson disease?\n",
            "  - 2. What are the different types of diagnostic tests used for Wilson disease?\n",
            "  - 3. How effective are these diagnostic tests in accurately identifying Wilson disease?\n",
            "  Retrieved 5 candidates in 5.592s\n",
            "\n",
            "[STEP 2] Re-Ranking with Cross-Encoder...\n",
            " Re-ranked 5 docs → top 5\n",
            "     1. Algorithms to facilitate diagnosis of symptomatic ... (score: 6.088)\n",
            "     2. DIAGNOSTIC TESTING... (score: 5.901)\n",
            "     3. DIAGNOSTIC TESTING... (score: 4.039)\n",
            "     4. Algorithms to facilitate diagnosis of symptomatic ... (score: -3.119)\n",
            "     5. DIAGNOSTIC TESTING... (score: -8.255)\n",
            "  Re-ranking completed in 0.010s\n",
            "\n",
            "[STEP 3] Generating Answer...\n",
            "  Answer generated in 9.367s\n",
            "1️⃣  PRECISION:  0.6000 (MQ) | 0.8000 (RR)\n",
            "2️⃣  NDCG:       0.9781 (MQ) | 0.9889 (RR)\n",
            "3️⃣  ROUGE:      0.1237 (MQ) | 0.2324 (RR) | +87.8% ↑\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Q2: What are the stages of liver fibrosis and their clinical significance?\n",
            "──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What are the diagnostic methods for determining the stages of liver fibrosis?\n",
            "  - 2. How does liver fibrosis progression affect the liver's function?\n",
            "  - 3. What are the treatment options for patients with advanced liver fibrosis?\n",
            "\n",
            "[STEP 1] Multi-Query Retrieval...\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What are the diagnostic criteria for each stage of liver fibrosis?\n",
            "  - 2. What are the treatment options available for each stage of liver fibrosis?\n",
            "  - 3. How does the progression of liver fibrosis vary between different types of AASLD liver disease?\n",
            "  Retrieved 7 candidates in 3.738s\n",
            "\n",
            "[STEP 2] Re-Ranking with Cross-Encoder...\n",
            " Re-ranked 7 docs → top 5\n",
            "     1. Stages of cirrhosis... (score: 5.869)\n",
            "     2. Histopathological principles underlying NILDA... (score: 2.340)\n",
            "     3. Step 1: Determine the stage of fibrosis... (score: 0.919)\n",
            "     4. HISTOPATHOLOGICAL PRINCIPLES UNDERLYING NILDA... (score: 0.701)\n",
            "     5. HISTOPATHOLOGICAL PRINCIPLES UNDERLYING NILDA... (score: 0.565)\n",
            "  Re-ranking completed in 0.012s\n",
            "\n",
            "[STEP 3] Generating Answer...\n",
            "  Answer generated in 9.612s\n",
            "1️⃣  PRECISION:  1.0000 (MQ) | 1.0000 (RR)\n",
            "2️⃣  NDCG:       0.9997 (MQ) | 0.9992 (RR)\n",
            "3️⃣  ROUGE:      0.3084 (MQ) | 0.3057 (RR) | -0.9% ↑\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Q3: What are the diagnostic features and complications of cirrhosis?\n",
            "──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - What are the risk factors for cirrhosis?\n",
            "  - How is cirrhosis diagnosed?\n",
            "  - What are the different types of cirrhosis?\n",
            "\n",
            "[STEP 1] Multi-Query Retrieval...\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What are the risk factors for the development of cirrhosis?\n",
            "  - 2. How is cirrhosis classified based on its underlying cause?\n",
            "  - 3. What are the treatment options available for cirrhosis?\n",
            "  Retrieved 9 candidates in 2.052s\n",
            "\n",
            "[STEP 2] Re-Ranking with Cross-Encoder...\n",
            " Re-ranked 9 docs → top 5\n",
            "     1. Cryptogenic Cirrhosis... (score: -2.011)\n",
            "     2. Stages of cirrhosis... (score: -5.746)\n",
            "     3. Compensated cirrhosis without CSPH but with mild P... (score: -6.268)\n",
            "     4. Emerging etiologic risk factors... (score: -7.854)\n",
            "     5. Target populations for HCC surveillance... (score: -8.955)\n",
            "  Re-ranking completed in 0.017s\n",
            "\n",
            "[STEP 3] Generating Answer...\n",
            "  Answer generated in 9.521s\n",
            "1️⃣  PRECISION:  0.8000 (MQ) | 0.4000 (RR)\n",
            "2️⃣  NDCG:       1.0000 (MQ) | 0.9998 (RR)\n",
            "3️⃣  ROUGE:      0.1606 (MQ) | 0.1511 (RR) | -5.9% ↑\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Q4: What factors determine eligibility for liver transplantation?\n",
            "──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What are the different types of liver diseases that may require transplantation?\n",
            "  - 2. How is liver transplantation performed and what are the potential risks and benefits?\n",
            "  - 3. What criteria are used to assess eligibility for liver transplantation?\n",
            "\n",
            "[STEP 1] Multi-Query Retrieval...\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. How does the liver function impact the determination of eligibility for transplantation?\n",
            "  - 2. What criteria are used to assess the severity of liver disease in order to determine eligibility for transplantation?\n",
            "  - 3. What is the role of liver function tests in determining eligibility for liver transplantation?\n",
            "  Retrieved 8 candidates in 2.029s\n",
            "\n",
            "[STEP 2] Re-Ranking with Cross-Encoder...\n",
            " Re-ranked 8 docs → top 5\n",
            "     1. Recommendations... (score: 0.517)\n",
            "     2. Contraindications to Liver Transplantation... (score: -2.369)\n",
            "     3. The Evaluation Process... (score: -2.620)\n",
            "     4. Recommendations... (score: -2.727)\n",
            "     5. Liver transplantation... (score: -2.862)\n",
            "  Re-ranking completed in 0.012s\n",
            "\n",
            "[STEP 3] Generating Answer...\n",
            "  Answer generated in 8.423s\n",
            "1️⃣  PRECISION:  0.2000 (MQ) | 0.0000 (RR)\n",
            "2️⃣  NDCG:       0.9654 (MQ) | 0.9815 (RR)\n",
            "3️⃣  ROUGE:      0.1853 (MQ) | 0.1508 (RR) | -18.6% ↑\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "Q5: What are the recommended screening methods for hepatocellular carcinoma?\n",
            "──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - a) How often should these screening methods be performed?\n",
            "  - b) What are the risk factors for hepatocellular carcinoma that should be taken into consideration during screening?\n",
            "  - c) Are there any specific symptoms that may indicate the presence of hepatocellular carcinoma and should prompt further screening?\n",
            "\n",
            "[STEP 1] Multi-Query Retrieval...\n",
            "[DEBUG] Generated 3 related questions:\n",
            "  - 1. What are the symptoms of hepatocellular carcinoma?\n",
            "  - 2. What are the risk factors for developing hepatocellular carcinoma?\n",
            "  - 3. What are the treatment options for hepatocellular carcinoma?\n",
            "  Retrieved 10 candidates in 1.675s\n",
            "\n",
            "[STEP 2] Re-Ranking with Cross-Encoder...\n",
            " Re-ranked 10 docs → top 5\n",
            "     1. Surgical resection for hepatocellular carcinoma: L... (score: 4.414)\n",
            "     2. Abstract... (score: 4.292)\n",
            "     3. Abstract... (score: 3.981)\n",
            "     4. Surgical resection... (score: 0.272)\n",
            "     5. AASLD Practice Guidance on the clinical assessment... (score: -0.143)\n",
            "  Re-ranking completed in 0.014s\n",
            "\n",
            "[STEP 3] Generating Answer...\n",
            "  Answer generated in 15.569s\n",
            "1️⃣  PRECISION:  0.6000 (MQ) | 0.6000 (RR)\n",
            "2️⃣  NDCG:       0.9831 (MQ) | 0.9882 (RR)\n",
            "3️⃣  ROUGE:      0.2333 (MQ) | 0.1405 (RR) | -39.8% ↑\n",
            "\n",
            "\n",
            "==================================================================================================================================\n",
            " FINAL RESULTS TABLE (7 Columns)\n",
            "==================================================================================================================================\n",
            " Q  Precision_MQ  Precision_RR  NDCG_MQ  NDCG_RR  ROUGE_MQ  ROUGE_RR  ROUGE_Improvement_%\n",
            " 1           0.6           0.8 0.978091 0.988899  0.123737  0.232412            87.826760\n",
            " 2           1.0           1.0 0.999699 0.999162  0.308401  0.305727            -0.866965\n",
            " 3           0.8           0.4 1.000000 0.999767  0.160582  0.151071            -5.922919\n",
            " 4           0.2           0.0 0.965441 0.981468  0.185297  0.150840           -18.595229\n",
            " 5           0.6           0.6 0.983064 0.988174  0.233260  0.140523           -39.757037\n",
            "\n",
            "\n",
            "==================================================================================================================================\n",
            " OVERALL SUMMARY (5 Questions Average)\n",
            "==================================================================================================================================\n",
            "\n",
            "Metric                    MultiQuery           ReRanker            \n",
            "─────────────────────────────────────────────────────────────────\n",
            "Precision@5               0.6400               0.5600              \n",
            "NDCG@5                    0.9853               0.9915              \n",
            "ROUGE (avg)               0.2023               0.1961              \n",
            "ROUGE Improvement                                              4.5%\n",
            "\n",
            "\n",
            "✓ Results exported to: final_evaluation_Qwen.csv\n",
            "\n",
            "==================================================================================================================================\n",
            "✓ FINAL EVALUATION COMPLETE\n",
            "==================================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ======================================\n",
        "# FINAL BATCH EVALUATION REPORT\n",
        "# 5 BALANCED QUESTIONS × 7 COLUMNS\n",
        "# ======================================\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "TAU = 0.65\n",
        "K = 5\n",
        "\n",
        "test_questions = [\n",
        "    {'num': 1, 'question': \"How is Wilson disease diagnosed and what are the diagnostic tests?\"},\n",
        "    {'num': 2, 'question': \"What are the stages of liver fibrosis and their clinical significance?\"},\n",
        "    {'num': 3, 'question': \"What are the diagnostic features and complications of cirrhosis?\"},\n",
        "    {'num': 4, 'question': \"What factors determine eligibility for liver transplantation?\"},\n",
        "    {'num': 5, 'question': \"What are the recommended screening methods for hepatocellular carcinoma?\"}\n",
        "]\n",
        "\n",
        "results_data = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*130)\n",
        "print(\"📊 FINAL BATCH EVALUATION: 5 BALANCED QUESTIONS × 7 METRICS\")\n",
        "print(\"=\"*130)\n",
        "print(\"Columns: Precision (MQ/RR) | NDCG (MQ/RR) | ROUGE (MQ/RR) | ROUGE Improvement %\")\n",
        "print(\"=\"*130)\n",
        "\n",
        "# Run evaluation\n",
        "for q_data in test_questions:\n",
        "    question = q_data['question']\n",
        "    q_num = q_data['num']\n",
        "\n",
        "    print(f\"\\n{'─'*130}\")\n",
        "    print(f\"Q{q_num}: {question}\")\n",
        "    print(f\"{'─'*130}\")\n",
        "\n",
        "    result_row = {'Q': q_num}\n",
        "\n",
        "    try:\n",
        "        # Generate answer ONCE for MultiQuery\n",
        "        result_mq = generate_answer_multiquery_v2(question, k_per_query=3, n_related=3, max_total=K)\n",
        "        answer_mq = result_mq.get(\"answer\", \"\")\n",
        "        sources_mq = result_mq.get(\"sources\", [])\n",
        "\n",
        "        # Generate answer ONCE for ReRanker\n",
        "        result_rr = generate_answer_with_rerank(question, k_per_query=3, n_related=3, max_retrieved=10, rerank_top_k=K)\n",
        "        answer_rr = result_rr.get(\"answer\", \"\")\n",
        "        sources_rr = result_rr.get(\"sources\", [])\n",
        "\n",
        "        # ===== PRECISION =====\n",
        "        qv = embedder.embed_query(question)\n",
        "\n",
        "        # Precision MQ\n",
        "        texts_mq = [doc.get('text', doc.get('page_content', '')) for doc in sources_mq[:K]]\n",
        "        if texts_mq:\n",
        "            embs_mq = np.array(embedder.embed_documents(texts_mq), dtype=np.float32)\n",
        "            sims_mq = cosine_similarity([qv], embs_mq)[0]\n",
        "            p_mq_score = sum(1 for sim in sims_mq if sim > TAU) / K\n",
        "        else:\n",
        "            p_mq_score = 0.0\n",
        "        result_row['Precision_MQ'] = p_mq_score\n",
        "\n",
        "        # Precision RR\n",
        "        texts_rr = [doc.get('text', doc.get('heading', '')) for doc in sources_rr[:K]]\n",
        "        if texts_rr:\n",
        "            embs_rr = np.array(embedder.embed_documents(texts_rr), dtype=np.float32)\n",
        "            sims_rr = cosine_similarity([qv], embs_rr)[0]\n",
        "            p_rr_score = sum(1 for sim in sims_rr if sim > TAU) / K\n",
        "        else:\n",
        "            p_rr_score = 0.0\n",
        "        result_row['Precision_RR'] = p_rr_score\n",
        "\n",
        "        # ===== NDCG =====\n",
        "        # NDCG MQ\n",
        "        if texts_mq:\n",
        "            gains_mq = sims_mq\n",
        "            dcg_mq = sum(g / np.log2(i+2) for i, g in enumerate(gains_mq))\n",
        "            ideal_mq = np.sort(gains_mq)[::-1]\n",
        "            idcg_mq = sum(g / np.log2(i+2) for i, g in enumerate(ideal_mq))\n",
        "            ndcg_mq_score = dcg_mq / idcg_mq if idcg_mq > 0 else 0.0\n",
        "        else:\n",
        "            ndcg_mq_score = 0.0\n",
        "        result_row['NDCG_MQ'] = ndcg_mq_score\n",
        "\n",
        "        # NDCG RR\n",
        "        if texts_rr:\n",
        "            gains_rr = sims_rr\n",
        "            dcg_rr = sum(g / np.log2(i+2) for i, g in enumerate(gains_rr))\n",
        "            ideal_rr = np.sort(gains_rr)[::-1]\n",
        "            idcg_rr = sum(g / np.log2(i+2) for i, g in enumerate(ideal_rr))\n",
        "            ndcg_rr_score = dcg_rr / idcg_rr if idcg_rr > 0 else 0.0\n",
        "        else:\n",
        "            ndcg_rr_score = 0.0\n",
        "        result_row['NDCG_RR'] = ndcg_rr_score\n",
        "\n",
        "        # ===== ROUGE =====\n",
        "        # ROUGE MQ\n",
        "        rouge_mq = metric_5_rouge_multiquery(answer_mq, sources_mq, question)\n",
        "        r_mq_score = rouge_mq.get('avg', 0.0)\n",
        "        result_row['ROUGE_MQ'] = r_mq_score\n",
        "\n",
        "        # ROUGE RR\n",
        "        rouge_rr = metric_5_rouge_reranker(answer_rr, sources_rr, question)\n",
        "        r_rr_score = rouge_rr.get('avg', 0.0)\n",
        "        result_row['ROUGE_RR'] = r_rr_score\n",
        "\n",
        "        # ROUGE Improvement %\n",
        "        rouge_improvement = ((r_rr_score - r_mq_score) / max(r_mq_score, 0.0001)) * 100\n",
        "        result_row['ROUGE_Improvement_%'] = rouge_improvement\n",
        "\n",
        "        print(f\"1️⃣  PRECISION:  {p_mq_score:.4f} (MQ) | {p_rr_score:.4f} (RR)\")\n",
        "        print(f\"2️⃣  NDCG:       {ndcg_mq_score:.4f} (MQ) | {ndcg_rr_score:.4f} (RR)\")\n",
        "        print(f\"3️⃣  ROUGE:      {r_mq_score:.4f} (MQ) | {r_rr_score:.4f} (RR) | {rouge_improvement:+.1f}% ↑\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error: {str(e)[:60]}\")\n",
        "        result_row['Precision_MQ'] = result_row['Precision_RR'] = 0.0\n",
        "        result_row['NDCG_MQ'] = result_row['NDCG_RR'] = 0.0\n",
        "        result_row['ROUGE_MQ'] = result_row['ROUGE_RR'] = 0.0\n",
        "        result_row['ROUGE_Improvement_%'] = 0.0\n",
        "\n",
        "    results_data.append(result_row)\n",
        "\n",
        "# Create DataFrame\n",
        "df_results = pd.DataFrame(results_data)\n",
        "\n",
        "# Print final results table\n",
        "print(\"\\n\\n\" + \"=\"*130)\n",
        "print(\" FINAL RESULTS TABLE (7 Columns)\")\n",
        "print(\"=\"*130)\n",
        "\n",
        "final_table = df_results[['Q', 'Precision_MQ', 'Precision_RR', 'NDCG_MQ', 'NDCG_RR', 'ROUGE_MQ', 'ROUGE_RR', 'ROUGE_Improvement_%']].copy()\n",
        "\n",
        "print(final_table.to_string(index=False))\n",
        "\n",
        "# Overall summary\n",
        "print(\"\\n\\n\" + \"=\"*130)\n",
        "print(\" OVERALL SUMMARY (5 Questions Average)\")\n",
        "print(\"=\"*130)\n",
        "\n",
        "print(f\"\\n{'Metric':<25} {'MultiQuery':<20} {'ReRanker':<20}\")\n",
        "print(\"─\" * 65)\n",
        "print(f\"{'Precision@5':<25} {df_results['Precision_MQ'].mean():<20.4f} {df_results['Precision_RR'].mean():<20.4f}\")\n",
        "print(f\"{'NDCG@5':<25} {df_results['NDCG_MQ'].mean():<20.4f} {df_results['NDCG_RR'].mean():<20.4f}\")\n",
        "print(f\"{'ROUGE (avg)':<25} {df_results['ROUGE_MQ'].mean():<20.4f} {df_results['ROUGE_RR'].mean():<20.4f}\")\n",
        "print(f\"{'ROUGE Improvement':<25} {'':<20} {df_results['ROUGE_Improvement_%'].mean():>19.1f}%\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = f\"final_evaluation_Qwen.csv\"\n",
        "df_results.to_csv(csv_filename, index=False)\n",
        "print(f\"\\n\\n✓ Results exported to: {csv_filename}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*130)\n",
        "print(\"✓ FINAL EVALUATION COMPLETE\")\n",
        "print(\"=\"*130)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6E4PMpUX0ies"
      },
      "execution_count": 74,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eef5d7c3e5144f158356fbd454648e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_824b31aace7444ca87aa71037de1e3bf",
              "IPY_MODEL_19e74aa0d51d44a095bfc5017182829f",
              "IPY_MODEL_422d42030b4e4ff49ecbfe35e15483cc"
            ],
            "layout": "IPY_MODEL_dc5ae7bda348476dbc22e715f1a77611"
          }
        },
        "824b31aace7444ca87aa71037de1e3bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4b94e581c93424990df5fa3d2ed4d5f",
            "placeholder": "​",
            "style": "IPY_MODEL_a823f2f2d1fe415991144070c89160a4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "19e74aa0d51d44a095bfc5017182829f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_394d6f9ebbec453ea34577672219f4f7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10787486b43d4125ae5031bc1bde208d",
            "value": 2
          }
        },
        "422d42030b4e4ff49ecbfe35e15483cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba6171f7bd6146789c6850c7fa892c39",
            "placeholder": "​",
            "style": "IPY_MODEL_7ca3c229dc804bbdb857cb09763be23f",
            "value": " 2/2 [00:28&lt;00:00, 12.21s/it]"
          }
        },
        "dc5ae7bda348476dbc22e715f1a77611": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4b94e581c93424990df5fa3d2ed4d5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a823f2f2d1fe415991144070c89160a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "394d6f9ebbec453ea34577672219f4f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10787486b43d4125ae5031bc1bde208d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba6171f7bd6146789c6850c7fa892c39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ca3c229dc804bbdb857cb09763be23f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}